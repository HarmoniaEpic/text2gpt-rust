# CLAUDE.md - Source Code Directory

## ğŸ“ Directory Purpose
Main source code directory containing all Rust implementation files for Text2GPT1.

## ğŸ—ï¸ Architecture Overview
```
src/
â”œâ”€â”€ main.rs          # Entry point, CLI implementation
â”œâ”€â”€ lib.rs           # Library root, public API
â”œâ”€â”€ config.rs        # Configuration structures
â”œâ”€â”€ model/           # Neural network implementation
â”œâ”€â”€ tokenizer/       # Text tokenization
â”œâ”€â”€ data/            # Dataset generation/processing
â”œâ”€â”€ training/        # Training loop and optimization
â”œâ”€â”€ inference/       # Text generation at inference
â”œâ”€â”€ io/              # File I/O, model serialization
â””â”€â”€ utils/           # Utility functions and helpers
```

## ğŸ”‘ Key Design Patterns

### Error Handling
```rust
use anyhow::{Result, Context};

// Preferred pattern
fn process_data(path: &Path) -> Result<Dataset> {
    let data = std::fs::read_to_string(path)
        .context("Failed to read dataset file")?;
    // ...
}
```

### Module Organization
- Each subdirectory has a `mod.rs` that re-exports public items
- Keep implementation details private, expose clean APIs
- Use `pub use` for convenience re-exports

### Type System
```rust
// Model sizes are strongly typed
pub enum ModelSize {
    Small,  // 12M
    Medium, // 33M
    Large,  // 117M
}

// Use newtype pattern for domain types
pub struct TokenCount(pub usize);
```

## ğŸš€ Entry Points

### CLI (main.rs)
- Interactive mode with dialoguer
- Subcommands: generate, infer, list
- Handles both Ollama and template-based generation

### Library API (lib.rs)
```rust
pub use config::{Config, ModelSize};
pub use model::GPT;
pub use tokenizer::GPT2Tokenizer;
```

## ğŸ§© Module Interactions
```
main.rs
  â”œâ”€> training/trainer.rs (run_full_pipeline)
  â”‚     â”œâ”€> data/generator.rs (generate samples)
  â”‚     â”œâ”€> data/refiner.rs (refine quality)
  â”‚     â”œâ”€> model/gpt.rs (create model)
  â”‚     â””â”€> io/safetensors.rs (save model)
  â”‚
  â””â”€> inference/generator.rs (text generation)
        â””â”€> model/gpt.rs (forward pass)
```

## ğŸ”§ Feature Flags
```toml
# Cargo.toml (planned)
[features]
default = ["cpu"]
cpu = []
cuda = ["candle/cuda"]
```

## ğŸ“ Common Tasks

### Adding a New Feature
1. Define interface in appropriate module
2. Implement core logic
3. Wire up in main.rs if user-facing
4. Add tests in same file or `tests/`

### Debugging Tips
- Use `RUST_LOG=debug` for verbose output
- Check `target/debug/` for intermediate files
- Use `cargo expand` to see macro expansions

## âš ï¸ Important Constraints
- **Candle Tensors**: Always specify device (CPU/CUDA)
- **Memory**: Watch for large tensors, use views when possible
- **Async**: Only in data generation (Ollama API calls)
- **Dependencies**: Minimize external crates

## ğŸ§ª Testing Strategy
```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_feature() {
        // Unit tests go in same file
    }
}
```

## ğŸ¤– AI Pair Programming Notes
- When adding features, consider both CPU and GPU paths
- Error messages should be helpful with `.context()`
- Keep public APIs minimal and well-documented
- Use `cargo clippy` suggestions

---
*This file helps AI assistants understand the source code organization.*