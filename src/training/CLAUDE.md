# CLAUDE.md - Training Pipeline

## ğŸ“ Directory Purpose
Complete training pipeline from data generation to saved model, including optimization and evaluation.

## ğŸ—ï¸ Module Structure
```
training/
â”œâ”€â”€ mod.rs       # Public exports
â””â”€â”€ trainer.rs   # Full pipeline implementation
```

## ğŸ”„ Training Pipeline Flow
```
run_full_pipeline()
    â”œâ”€> 1. Initialize tokenizer
    â”œâ”€> 2. Generate data (DataGenerator)
    â”œâ”€> 3. Refine data (DataRefiner)
    â”œâ”€> 4. Create dataset (TextDataset)
    â”œâ”€> 5. Initialize model (GPT)
    â”œâ”€> 6. Train model (train_model)
    â”œâ”€> 7. Test generation
    â””â”€> 8. Save everything
         â”œâ”€> model.safetensors
         â”œâ”€> tokenizer.json
         â”œâ”€> config.json
         â”œâ”€> dataset.json/txt
         â””â”€> generation_info.json
```

## ğŸ¯ Key Functions

### Main Pipeline Entry
```rust
pub async fn run_full_pipeline(
    prompt: &str,
    config: &Config,
    output_dir: &Path,
    generation_method: DataGenerationMethod,
) -> Result<TrainingResult>
```

### Training Loop
```rust
pub fn train_model(
    model: &GPT,
    dataset: &TextDataset,
    config: &Config,
    varmap: &Arc<VarMap>,
) -> Result<f32>
```

## ğŸ”§ Training Configuration

### Optimizer Settings
```rust
AdamW {
    lr: 3e-4,          // Learning rate
    beta1: 0.9,        // First moment decay
    beta2: 0.999,      // Second moment decay
    eps: 1e-8,         // Epsilon
    weight_decay: 0.01, // L2 regularization
}
```

### Default Training Parameters
```rust
epochs: 20            // Can be customized
batch_size: 2         // Memory-efficient default
max_length: 128       // Sequence length
dropout: 0.1          // During training only
```

### Learning Schedule
- Currently: Fixed learning rate
- TODO: Cosine annealing or warmup

## ğŸ“Š Training Monitoring

### Progress Display
```
ğŸ”„ [00:05:23] [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 156/200 steps | Loss: 3.4521 | ETA: 00:02:15
```

### Loss Tracking
- Smoothed loss over last 10 batches
- Epoch averages logged every 5 epochs
- Final loss returned for evaluation

### Memory Optimization
```rust
// Gradient accumulation (if needed)
// Currently: Direct update each batch
// TODO: Add accumulation for larger effective batch
```

## ğŸ’¾ Model Saving

### Directory Structure
```
models/cooking_recipe_generator_20240123_143022/
â”œâ”€â”€ model.safetensors      # Model weights (VarMap)
â”œâ”€â”€ tokenizer.json         # GPT-2 tokenizer
â”œâ”€â”€ config.json           # Model configuration
â”œâ”€â”€ generation_info.json  # Training metadata
â”œâ”€â”€ dataset.json          # Training data (JSON)
â””â”€â”€ dataset.txt           # Training data (text)
```

### Folder Naming
```rust
// Using FolderNameBuilder
"cooking_recipe_generator_20240123_143022"
  â†‘ domain/preset          â†‘ timestamp
```

## ğŸ§ª Post-Training Tests

### Generation Tests
```rust
// Test prompts
["", "The", "Today"]

// Parameters
max_tokens: 30
temperature: 0.8
top_k: 40
```

## âš ï¸ Common Training Issues

### Out of Memory
```rust
// Solutions:
1. Reduce batch_size
2. Use smaller model_size
3. Reduce max_length
4. Enable gradient checkpointing (TODO)
```

### Loss Not Decreasing
```rust
// Check:
1. Learning rate (try 1e-4 to 1e-3)
2. Data quality (refiner thresholds)
3. Model initialization
4. Sufficient data amount
```

### Slow Training
```rust
// Optimize:
1. Ensure CUDA is used: config.device
2. Increase batch_size if memory allows
3. Use larger but fewer sequences
4. Profile with flamegraph
```

## ğŸ“ˆ Training Metrics

### Expected Loss Curves
```
Small (12M):
  Initial: ~8-10
  Final: ~2-4

Medium (33M):
  Initial: ~8-10
  Final: ~1.5-3

Large (117M):
  Initial: ~8-10
  Final: ~1-2.5
```

### Training Time Estimates
```
GPU (RTX 3080):
  12M model, 20 epochs: ~10 minutes
  33M model, 20 epochs: ~20 minutes
  117M model, 20 epochs: ~40 minutes

CPU:
  Multiply by ~10-20x
```

## ğŸ”„ Recent Improvements
- Added VarMap for better weight management
- Implemented proper loss smoothing
- Added progress bar with ETA
- Improved error messages with context

## ğŸ¤– AI Collaboration Notes

### Current Limitations
- No learning rate scheduling
- No gradient clipping
- No validation set splitting
- No early stopping

### Future Enhancements
- [ ] Mixed precision training (f16)
- [ ] Gradient accumulation
- [ ] Checkpoint resumption
- [ ] Distributed training support
- [ ] TensorBoard logging

### Code Patterns
```rust
// Always use context for errors
.context("Failed during model training")?

// Candle-specific optimizer pattern
optimizer.backward_step(&loss_tensor)?;

// Progress reporting pattern
pb.set_message(format!("{:.4}", smoothed_loss));
```

---
*This file documents the training pipeline implementation.*