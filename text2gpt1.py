#!/usr/bin/env python3
"""
Text2GPT1 Textualç‰ˆ: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‰¹å®šã®æ€§è³ªã‚’æŒã¤GPT-1ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ
- Textualãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ€ãƒ³ãªãƒªãƒƒãƒ TUI
- ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ç®¡ç†æ©Ÿèƒ½
- ã‚«ãƒ†ã‚´ãƒªé¸æŠUIï¼ˆCooking, Poetry, Technical, Generalï¼‰
- Ollamaçµ±åˆã«ã‚ˆã‚‹LLMãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- 1kãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- 12Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-1ã®1/10ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
- safetensorså½¢å¼ã§ã®ä¿å­˜
- ä½œæˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®æ¨è«–æ©Ÿèƒ½
"""

import os
import sys
import json
import random
import re
import time
import argparse
import requests
import asyncio
import unicodedata
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
from safetensors.torch import save_file, load_file
from transformers import GPT2Tokenizer
import warnings
warnings.filterwarnings('ignore')

# Textual imports
from textual import on, work
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Horizontal, Vertical, ScrollableContainer
from textual.css.query import NoMatches
from textual.message import Message
from textual.reactive import reactive
from textual.screen import Screen, ModalScreen
from textual.widgets import (
    Button, DataTable, Footer, Header, Input, Label, ListView, ListItem,
    LoadingIndicator, Log, OptionList, ProgressBar, RadioButton, RadioSet,
    RichLog, Static, TabbedContent, TabPane, TextArea
)
from textual.worker import Worker, get_current_worker
from rich.text import Text
from rich.panel import Panel
from rich.table import Table
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn


# ===== ã‚«ãƒ†ã‚´ãƒªã¨ãƒ—ãƒªã‚»ãƒƒãƒˆã®å®šç¾© =====
CATEGORIES = {
    "cooking": {
        "name": "ğŸ³ Cooking & Recipes",
        "name_ja": "æ–™ç†ãƒ»ãƒ¬ã‚·ãƒ”",
        "description": "æ–™ç†ãƒ¬ã‚·ãƒ”ã‚„é£Ÿæã®èª¬æ˜ã‚’ç”Ÿæˆ",
        "icon": "ğŸ³",
        "presets": [
            {
                "title": "å®¶åº­æ–™ç†ãƒ¬ã‚·ãƒ”ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼",
                "title_en": "cooking_recipe_generator",
                "prompt": "ç°¡å˜ã§ç¾å‘³ã—ã„å®¶åº­æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹æ¥½ã—ã„GPT",
                "description": "æ—¥å¸¸çš„ãªé£Ÿæã§ä½œã‚Œã‚‹æ–™ç†ãƒ¬ã‚·ãƒ”ã‚’æä¾›"
            },
            {
                "title": "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«æ–™ç†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
                "title_en": "professional_chef_assistant",
                "prompt": "æœ¬æ ¼çš„ãªæ–™ç†æŠ€æ³•ã¨é«˜åº¦ãªãƒ¬ã‚·ãƒ”ã‚’æä¾›ã™ã‚‹å°‚é–€çš„ãªGPT",
                "description": "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³å“è³ªã®æ–™ç†ã‚’è§£èª¬"
            },
            {
                "title": "ãƒ˜ãƒ«ã‚·ãƒ¼ï¼†ãƒ€ã‚¤ã‚¨ãƒƒãƒˆãƒ¬ã‚·ãƒ”",
                "title_en": "healthy_diet_recipes",
                "prompt": "å¥åº·çš„ã§æ „é¤Šãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹ãƒ˜ãƒ«ã‚·ãƒ¼GPT",
                "description": "ã‚«ãƒ­ãƒªãƒ¼æ§ãˆã‚ã§æ „é¤Šè±Šå¯Œãªãƒ¬ã‚·ãƒ”"
            },
            {
                "title": "ãŠè“å­ãƒ»ãƒ‡ã‚¶ãƒ¼ãƒˆå°‚é–€",
                "title_en": "sweets_dessert_specialist",
                "prompt": "ã‚¹ã‚¤ãƒ¼ãƒ„ã‚„ãƒ‡ã‚¶ãƒ¼ãƒˆã®ãƒ¬ã‚·ãƒ”ã«ç‰¹åŒ–ã—ãŸç”˜ã„GPT",
                "description": "ã‚±ãƒ¼ã‚­ã€ã‚¯ãƒƒã‚­ãƒ¼ã€å’Œè“å­ãªã©"
            }
        ]
    },
    "poetry": {
        "name": "âœï¸ Poetry & Creative",
        "name_ja": "è©©ãƒ»å‰µä½œ",
        "description": "è©©çš„ã§å‰µé€ çš„ãªæ–‡ç« ã‚’ç”Ÿæˆ",
        "icon": "âœï¸",
        "presets": [
            {
                "title": "ç¾ä»£è©©ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼",
                "title_en": "modern_poetry_generator",
                "prompt": "ç¾ä»£çš„ã§æ„Ÿæ€§è±Šã‹ãªè©©ã‚’å‰µä½œã™ã‚‹è©©äººGPT",
                "description": "è‡ªç”±è©©ã‚„æ•£æ–‡è©©ã‚’ç”Ÿæˆ"
            },
            {
                "title": "ä¿³å¥ãƒ»çŸ­æ­Œãƒã‚¹ã‚¿ãƒ¼",
                "title_en": "haiku_tanka_master",
                "prompt": "æ—¥æœ¬ã®ä¼çµ±çš„ãªè©©å½¢ã‚’ç”Ÿæˆã™ã‚‹å’Œé¢¨GPT",
                "description": "5-7-5ã‚„5-7-5-7-7ã®å½¢å¼"
            },
            {
                "title": "ç‰©èªãƒ»å°èª¬ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼",
                "title_en": "story_novel_creator",
                "prompt": "çŸ­ç·¨å°èª¬ã‚„ç‰©èªã‚’å‰µä½œã™ã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒ©ãƒ¼GPT",
                "description": "å‰µé€ çš„ãªç‰©èªã‚’ç´¡ã"
            },
            {
                "title": "æ­Œè©ãƒ»ã‚½ãƒ³ã‚°ãƒ©ã‚¤ã‚¿ãƒ¼",
                "title_en": "lyrics_songwriter",
                "prompt": "æ„Ÿå‹•çš„ãªæ­Œè©ã‚’æ›¸ãéŸ³æ¥½çš„ãªGPT",
                "description": "æ§˜ã€…ãªã‚¸ãƒ£ãƒ³ãƒ«ã®æ­Œè©ã‚’å‰µä½œ"
            }
        ]
    },
    "technical": {
        "name": "ğŸ’» Technical & Code",
        "name_ja": "æŠ€è¡“ãƒ»ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
        "description": "æŠ€è¡“æ–‡æ›¸ã‚„ã‚³ãƒ¼ãƒ‰è§£èª¬ã‚’ç”Ÿæˆ",
        "icon": "ğŸ’»",
        "presets": [
            {
                "title": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è§£èª¬è€…",
                "title_en": "programming_explainer",
                "prompt": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®æ¦‚å¿µã¨ã‚³ãƒ¼ãƒ‰ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹GPT",
                "description": "åˆå¿ƒè€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„æŠ€è¡“è§£èª¬"
            },
            {
                "title": "APIãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ",
                "title_en": "api_documentation_writer",
                "prompt": "æŠ€è¡“ä»•æ§˜æ›¸ã‚„APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹æŠ€è¡“ãƒ©ã‚¤ã‚¿ãƒ¼GPT",
                "description": "æ§‹é€ åŒ–ã•ã‚ŒãŸæŠ€è¡“æ–‡æ›¸"
            },
            {
                "title": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ»ãƒ‡ãƒ¼ã‚¿æ§‹é€ ",
                "title_en": "algorithm_data_structures",
                "prompt": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’è§£èª¬ã™ã‚‹ç†è«–çš„ãªGPT",
                "description": "è¨ˆç®—æ©Ÿç§‘å­¦ã®åŸºç¤æ¦‚å¿µ"
            },
            {
                "title": "ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£",
                "title_en": "system_design_architecture",
                "prompt": "ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’èª¬æ˜ã™ã‚‹GPT",
                "description": "å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆæ€æƒ³"
            }
        ]
    },
    "general": {
        "name": "ğŸ“ General Purpose",
        "name_ja": "æ±ç”¨",
        "description": "æ§˜ã€…ãªç”¨é€”ã«å¯¾å¿œã™ã‚‹æ±ç”¨çš„ãªæ–‡ç« ç”Ÿæˆ",
        "icon": "ğŸ“",
        "presets": [
            {
                "title": "æ—¥å¸¸ä¼šè©±ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
                "title_en": "daily_conversation_assistant",
                "prompt": "è¦ªã—ã¿ã‚„ã™ã„æ—¥å¸¸ä¼šè©±ã‚’ç”Ÿæˆã™ã‚‹å‹é”ã®ã‚ˆã†ãªGPT",
                "description": "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå¯¾è©±ã‚„é›‘è«‡"
            },
            {
                "title": "ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ä½œæˆ",
                "title_en": "business_document_writer",
                "prompt": "ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ã‚„ãƒ¡ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«GPT",
                "description": "ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªãƒ“ã‚¸ãƒã‚¹æ–‡ç« "
            },
            {
                "title": "æ•™è‚²ãƒ»å­¦ç¿’ã‚µãƒãƒ¼ãƒˆ",
                "title_en": "education_learning_support",
                "prompt": "åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã§å­¦ç¿’ã‚’æ”¯æ´ã™ã‚‹æ•™è‚²çš„ãªGPT",
                "description": "æ§˜ã€…ãªåˆ†é‡ã®å­¦ç¿’æ”¯æ´"
            },
            {
                "title": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æƒ…å ±è¦ç´„",
                "title_en": "news_information_summary",
                "prompt": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„æƒ…å ±ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹è¦ç´„GPT",
                "description": "é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"
            }
        ]
    }
}

# Ollamaãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜æƒ…å ±
OLLAMA_MODEL_INFO = {
    "llama3": {
        "name": "Llama 3",
        "size": "8B",
        "description": "Meta's latest model - ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„æ±ç”¨ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "cooking", "poetry"]
    },
    "llama3:70b": {
        "name": "Llama 3 Large",
        "size": "70B",
        "description": "æœ€é«˜å“è³ªã®ç”Ÿæˆãƒ»è©•ä¾¡ï¼ˆè¦é«˜æ€§èƒ½GPUï¼‰",
        "best_for": ["technical", "poetry"]
    },
    "mistral": {
        "name": "Mistral",
        "size": "7B",
        "description": "é«˜é€Ÿã§åŠ¹ç‡çš„ãªç”Ÿæˆ",
        "best_for": ["general", "technical"]
    },
    "gemma": {
        "name": "Gemma",
        "size": "2B",
        "description": "Googleè£½ã®è»½é‡ãƒ¢ãƒ‡ãƒ« - é«˜é€Ÿå‡¦ç†",
        "best_for": ["general", "cooking"]
    },
    "gemma:7b": {
        "name": "Gemma Large",
        "size": "7B",
        "description": "Gemmaã®å¤§è¦æ¨¡ç‰ˆ - ã‚ˆã‚Šé«˜å“è³ª",
        "best_for": ["general", "technical"]
    },
    "gemma2:2b": {
        "name": "Gemma 2",
        "size": "2B",
        "description": "Googleè£½ã®æœ€æ–°è»½é‡ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "cooking"]
    },
    "gemma3:latest": {
        "name": "Gemma 3",
        "size": "Latest",
        "description": "Googleè£½ã®æœ€æ–°ç‰ˆGemmaãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "technical"]
    },
    "qwen3:1.7b": {
        "name": "Qwen 3",
        "size": "1.7B",
        "description": "Alibabaè£½ã®è»½é‡é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "technical"]
    },
    "codellama": {
        "name": "Code Llama",
        "size": "7B",
        "description": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»æŠ€è¡“æ–‡æ›¸ã«ç‰¹åŒ–",
        "best_for": ["technical"]
    },
    "phi": {
        "name": "Phi-2",
        "size": "2.7B",
        "description": "Microsoftè£½ã®å°å‹é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "technical"]
    },
    "neural-chat": {
        "name": "Neural Chat",
        "size": "7B",
        "description": "Intelè£½ã®å¯¾è©±ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "poetry"]
    },
    "starling-lm": {
        "name": "Starling",
        "size": "7B",
        "description": "Berkeleyè£½ã®æŒ‡ç¤ºè¿½å¾“ãƒ¢ãƒ‡ãƒ«",
        "best_for": ["general", "cooking"]
    },
    "orca-mini": {
        "name": "Orca Mini",
        "size": "3B",
        "description": "Microsoft Orcaã®è»½é‡ç‰ˆ",
        "best_for": ["general"]
    }
}


# ===== 1. è¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ =====
class Config:
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨­å®šã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆ12Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼šGPT-1ã®1/10ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    vocab_size = 50257  # GPT-2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°
    n_embd = 384       # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆGPT-1ã®768ã®åŠåˆ†ï¼‰
    n_layer = 6        # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆGPT-1ã®12ã®åŠåˆ†ï¼‰
    n_head = 6         # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°
    n_positions = 512  # æœ€å¤§ç³»åˆ—é•·
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    batch_size = 2     # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚º
    learning_rate = 3e-4
    num_epochs = 20    # é«˜é€Ÿãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ç”¨ã®ç¾å®Ÿçš„ãªã‚¨ãƒãƒƒã‚¯æ•°
    
    # ãƒ‡ãƒ¼ã‚¿è¨­å®š
    initial_tokens = 2000  # åˆæœŸç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
    final_tokens = 1000    # ç²¾é¸å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    max_length = 128       # å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®æœ€å¤§é•·
    
    # ãã®ä»–
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    seed = 42


# ===== ãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def normalize_folder_name(name: str, max_length: int = 50) -> str:
    """ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ­£è¦åŒ–ï¼ˆæ—¥æœ¬èªâ†’è‹±èªã€ç‰¹æ®Šæ–‡å­—å‡¦ç†ï¼‰"""
    # NFKDã§åˆ†è§£ã—ã¦ã€ã‚¢ã‚¯ã‚»ãƒ³ãƒˆè¨˜å·ã‚’é™¤å»
    name = unicodedata.normalize('NFKD', name)
    name = ''.join([c for c in name if not unicodedata.combining(c)])
    
    # è‹±æ•°å­—ã¨ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»¥å¤–ã‚’ç½®æ›
    name = re.sub(r'[^a-zA-Z0-9_\s-]', '', name)
    
    # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚„ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ä¸€ã¤ã«
    name = re.sub(r'[\s_-]+', '_', name)
    
    # å°æ–‡å­—åŒ–
    name = name.lower()
    
    # å‰å¾Œã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å‰Šé™¤
    name = name.strip('_')
    
    # æœ€å¤§é•·åˆ¶é™
    if len(name) > max_length:
        name = name[:max_length]
    
    # ç©ºã«ãªã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not name:
        name = "custom_model"
    
    return name


def create_model_folder_name(prompt: str, preset_info: Optional[Dict] = None) -> str:
    """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if preset_info and 'title_en' in preset_info:
        # ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨æ™‚
        base_name = preset_info['title_en']
    else:
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ™‚
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ä¸»è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        if len(prompt) > 30:
            # æœ€åˆã®30æ–‡å­—ã‹ã‚‰ç”Ÿæˆ
            base_name = "custom_" + normalize_folder_name(prompt[:30])
        else:
            base_name = "custom_" + normalize_folder_name(prompt)
    
    return f"{base_name}_{timestamp}"


# ===== 2. Textual ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å®šç¾© =====
@dataclass
class MainMenuClicked(Message):
    """ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    action: str  # "create" or "inference" or "list"


@dataclass
class ModelCardClicked(Message):
    """ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆæ¨è«–ç”¨ï¼‰"""
    model_path: str
    model_info: Dict


@dataclass
class CategoryCardClicked(Message):
    """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ¼ãƒ‰ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    category_key: str


@dataclass
class PresetCardClicked(Message):
    """ãƒ—ãƒªã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    preset_data: Dict
    

@dataclass
class EpochOptionClicked(Message):
    """ã‚¨ãƒãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    epochs: int


@dataclass
class GenerationMethodClicked(Message):
    """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    method: str  # "template" or "ollama"


@dataclass
class ModelCardOllamaClicked(Message):
    """Ollamaãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚¯ãƒªãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    model_name: str
    purpose: str  # "generation" or "refinement"


@dataclass
class ProgressUpdate(Message):
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    step_key: str
    current: Optional[int] = None
    total: Optional[int] = None
    info: Optional[str] = None


@dataclass
class PreviewUpdate(Message):
    """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    content: Any


@dataclass
class SystemInfoUpdate(Message):
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    key: str
    value: str


@dataclass
class StepChange(Message):
    """ã‚¹ãƒ†ãƒƒãƒ—å¤‰æ›´ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    step: int


@dataclass
class ProcessComplete(Message):
    """å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    folder_path: str
    domain: str


@dataclass
class ProcessError(Message):
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    error: str


# ===== 3. Textual ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ =====
class MainMenuCard(Static):
    """ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, action: str, title: str, description: str, icon: str, **kwargs):
        super().__init__(**kwargs)
        self.action = action
        self.title = title
        self.description = description
        self.icon = icon
        
    def compose(self) -> ComposeResult:
        yield Label(f"{self.icon} {self.title}", classes="menu-title")
        yield Label(self.description, classes="menu-description")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(MainMenuClicked(self.action))


class SavedModelCard(Static):
    """ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«é¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, model_path: str, model_info: Dict, **kwargs):
        super().__init__(**kwargs)
        self.model_path = model_path
        self.model_info = model_info
        
    def compose(self) -> ComposeResult:
        folder_name = os.path.basename(self.model_path)
        yield Label(f"ğŸ“ {folder_name}", classes="model-folder-name")
        yield Label(f"Domain: {self.model_info.get('category', 'Unknown')}", classes="model-info")
        yield Label(f"Prompt: {self.model_info.get('prompt', 'Unknown')[:50]}...", classes="model-info")
        yield Label(f"Size: {self.model_info.get('model_size', 'Unknown')}", classes="model-info")
        yield Label(f"Created: {self.model_info.get('creation_date', 'Unknown')}", classes="model-info")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(ModelCardClicked(self.model_path, self.model_info))


class CategoryCard(Static):
    """ã‚«ãƒ†ã‚´ãƒªé¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, category_key: str, category_data: Dict, **kwargs):
        super().__init__(**kwargs)
        self.category_key = category_key
        self.category_data = category_data
        
    def compose(self) -> ComposeResult:
        icon = self.category_data['icon']
        name = self.category_data['name']
        name_ja = self.category_data['name_ja']
        description = self.category_data['description']
        
        yield Label(f"{icon} {name}", classes="category-title")
        yield Label(f"({name_ja})", classes="category-subtitle")
        yield Label(description, classes="category-description")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(CategoryCardClicked(self.category_key))


class PresetCard(Static):
    """ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, preset_data: Dict, index: int, **kwargs):
        super().__init__(**kwargs)
        self.preset_data = preset_data
        self.index = index
        
    def compose(self) -> ComposeResult:
        yield Label(f"{self.index}. {self.preset_data['title']}", classes="preset-title")
        yield Label(f"ã€Œ{self.preset_data['prompt']}ã€", classes="preset-prompt")
        yield Label(self.preset_data['description'], classes="preset-description")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(PresetCardClicked(self.preset_data))


class EpochOption(Static):
    """ã‚¨ãƒãƒƒã‚¯æ•°é¸æŠã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    
    def __init__(self, epochs: int, epoch_name: str, description: str, time_estimate: str, **kwargs):
        super().__init__(**kwargs)
        self.epochs = epochs
        self.epoch_name = epoch_name
        self.description = description
        self.time_estimate = time_estimate
        
    def compose(self) -> ComposeResult:
        if self.epochs > 0:
            yield Label(f"{self.epochs} epochs - {self.epoch_name}", classes="epoch-title")
        else:
            yield Label(self.epoch_name, classes="epoch-title")
        yield Label(self.description, classes="epoch-description")
        yield Label(f"äºˆæƒ³æ™‚é–“: {self.time_estimate}", classes="epoch-time")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(EpochOptionClicked(self.epochs))


class GenerationMethodCard(Static):
    """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•é¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, method: str, title: str, description: str, advantages: List[str], **kwargs):
        super().__init__(**kwargs)
        self.method = method
        self.title = title
        self.description = description
        self.advantages = advantages
        
    def compose(self) -> ComposeResult:
        yield Label(self.title, classes="method-title")
        yield Label(self.description, classes="method-description")
        
        with Container(classes="advantages-list"):
            for advantage in self.advantages:
                yield Label(f"â€¢ {advantage}", classes="method-advantage")
        
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
        self.post_message(GenerationMethodClicked(self.method))


class ModelCardOllama(Static):
    """Ollamaãƒ¢ãƒ‡ãƒ«é¸æŠç”¨ã‚«ãƒ¼ãƒ‰"""
    
    def __init__(self, model_name: str, model_info: Dict, available: bool, recommended: bool, purpose: str, **kwargs):
        super().__init__(**kwargs)
        self.model_name = model_name
        self.model_info = model_info
        self.available = available
        self.recommended = recommended
        self.purpose = purpose
        
    def compose(self) -> ComposeResult:
        status = "âœ“" if self.available else "Ã—"
        recommend_mark = " â­" if self.recommended else ""
        
        title = f"[{'green' if self.available else 'red'}]{status}[/] {self.model_info['name']} ({self.model_info['size']}){recommend_mark}"
        yield Label(title, classes="model-title")
        yield Label(self.model_info['description'], classes="model-description")
        
        if not self.available:
            yield Label("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦: ollama pull " + self.model_name, classes="model-install-hint")
    
    def on_click(self) -> None:
        """ã‚¯ãƒªãƒƒã‚¯æ™‚ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰"""
        if self.available:
            self.post_message(ModelCardOllamaClicked(self.model_name, self.purpose))


# ===== 4. ç”»é¢å®šç¾© =====
class MainMenuScreen(Screen):
    """ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç”»é¢"""
    
    CSS = """
    MainMenuScreen {
        align: center middle;
    }
    
    #menu-container {
        width: 80;
        height: auto;
        border: solid $primary;
        padding: 2;
    }
    
    .menu-card {
        margin: 2;
        padding: 2;
        border: solid $surface;
    }
    
    .menu-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .menu-title {
        text-style: bold;
        color: $primary;
    }
    
    .menu-description {
        margin-top: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    """
    
    def compose(self) -> ComposeResult:
        yield Header()
        
        with Container(id="menu-container"):
            yield Label("Text2GPT1 - ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼", classes="title")
            
            # æ–°è¦ä½œæˆ
            card1 = MainMenuCard(
                "create",
                "æ–°ã—ã„GPTãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ",
                "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‰¹å®šã®æ€§è³ªã‚’æŒã¤GPTãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™",
                "ğŸ”¨",
                classes="menu-card"
            )
            card1.can_focus = True
            yield card1
            
            # æ¨è«–
            card2 = MainMenuCard(
                "inference",
                "æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–",
                "ä½œæˆæ¸ˆã¿ã®GPTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™",
                "ğŸ¤–",
                classes="menu-card"
            )
            card2.can_focus = True
            yield card2
                
        with Horizontal(id="button-container"):
            yield Button("çµ‚äº†", id="quit", variant="error")
            
        yield Footer()
        
    def on_mount(self) -> None:
        self.query_one(".menu-card").focus()
        
    async def on_main_menu_clicked(self, message: MainMenuClicked) -> None:
        """ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚«ãƒ¼ãƒ‰ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        if message.action == "create":
            await self.app.push_screen(CategorySelectionScreen())
        elif message.action == "inference":
            await self.app.push_screen(ModelSelectionScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "quit":
            self.app.exit()


class ModelSelectionScreen(Screen):
    """ãƒ¢ãƒ‡ãƒ«é¸æŠç”»é¢ï¼ˆæ¨è«–ç”¨ï¼‰"""
    
    CSS = """
    ModelSelectionScreen {
        align: center middle;
    }
    
    #model-container {
        width: 100;
        height: 80%;
        border: solid $primary;
        padding: 2;
    }
    
    .model-card {
        margin: 2;
        padding: 2;
        border: solid $surface;
    }
    
    .model-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .model-folder-name {
        text-style: bold;
        color: $primary;
    }
    
    .model-info {
        margin-top: 1;
        color: $text-muted;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    
    .no-models {
        text-align: center;
        color: $warning;
        margin: 4;
    }
    """
    
    def compose(self) -> ComposeResult:
        yield Header()
        
        with Container(id="model-container"):
            yield Label("ä½œæˆæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§", classes="title")
            
            with ScrollableContainer(id="model-list"):
                yield LoadingIndicator(id="loading")
                
        with Horizontal(id="button-container"):
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        """ç”»é¢ãƒã‚¦ãƒ³ãƒˆæ™‚ã«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’èª­ã¿è¾¼ã¿"""
        self.load_models()
        
    @work(thread=True)
    def load_models(self) -> None:
        """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        models = list_saved_models(self.app.args.output_dir)
        self.app.call_from_thread(self.display_models, models)
        
    def display_models(self, models: List[Dict]) -> None:
        """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã‚’å‰Šé™¤
        try:
            self.query_one("#loading").remove()
        except:
            pass
        
        model_list = self.query_one("#model-list")
        
        if not models:
            model_list.mount(Label(
                "ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\nå…ˆã«æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
                classes="no-models"
            ))
        else:
            for model in models:
                card = SavedModelCard(
                    model['path'],
                    model['info'],
                    classes="model-card"
                )
                card.can_focus = True
                model_list.mount(card)
                
    async def on_model_card_clicked(self, message: ModelCardClicked) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        self.app.selected_model_path = message.model_path
        self.app.selected_model_info = message.model_info
        await self.app.push_screen(InferenceScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "back":
            self.app.pop_screen()


class InferenceScreen(Screen):
    """æ¨è«–ç”»é¢"""
    
    BINDINGS = [
        Binding("ctrl+enter", "generate", "ç”Ÿæˆ"),
        Binding("ctrl+l", "clear", "ã‚¯ãƒªã‚¢"),
        Binding("escape", "back", "æˆ»ã‚‹"),
    ]
    
    CSS = """
    InferenceScreen {
        layout: grid;
        grid-size: 2 1;
        grid-columns: 2fr 3fr;
    }
    
    #left-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    #right-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    .panel-title {
        text-style: bold;
        color: $primary;
        margin-bottom: 1;
    }
    
    #model-info {
        margin-bottom: 2;
    }
    
    .param-container {
        margin: 1 0;
    }
    
    .param-label {
        margin-bottom: 1;
    }
    
    Input {
        width: 100%;
    }
    
    #prompt-area {
        height: 10;
        margin: 1 0;
    }
    
    #result-area {
        height: 20;
        margin: 1 0;
        border: solid $surface;
        padding: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
        margin-top: 1;
    }
    
    Button {
        margin: 0 1;
    }
    """
    
    def __init__(self):
        super().__init__()
        self.model = None
        self.tokenizer = None
        self.config = None
        self.temperature = reactive(0.8)
        self.max_length = reactive(100)
        self.top_k = reactive(40)
        
    def compose(self) -> ComposeResult:
        yield Header()
        
        # å·¦ãƒ‘ãƒãƒ« - è¨­å®š
        with Container(id="left-panel"):
            yield Label("âš™ï¸ è¨­å®š", classes="panel-title")
            
            with Container(id="model-info"):
                yield Label("èª­ã¿è¾¼ã¿ä¸­...", id="model-name")
                yield Label("", id="model-domain")
                yield Label("", id="model-size")
            
            # Temperature
            with Container(classes="param-container"):
                yield Label("Temperature (0.1-2.0):", classes="param-label")
                yield Input(value="0.8", id="temperature-input", type="number")
                
            # Max Length
            with Container(classes="param-container"):
                yield Label("Max Length (10-500):", classes="param-label")
                yield Input(value="100", id="max-length-input", type="integer")
                
            # Top-k
            with Container(classes="param-container"):
                yield Label("Top-k (1-100):", classes="param-label")
                yield Input(value="40", id="top-k-input", type="integer")
                
        # å³ãƒ‘ãƒãƒ« - ç”Ÿæˆ
        with Container(id="right-panel"):
            yield Label("âœï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ", classes="panel-title")
            
            yield Label("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:", classes="param-label")
            yield TextArea(id="prompt-area", language="markdown")
            
            yield Label("ç”Ÿæˆçµæœ:", classes="param-label")
            yield RichLog(highlight=True, markup=True, id="result-area")
            
            with Horizontal(id="button-container"):
                yield Button("ç”Ÿæˆ", id="generate", variant="primary")
                yield Button("ã‚¯ãƒªã‚¢", id="clear")
                yield Button("åˆ¥ã®ãƒ¢ãƒ‡ãƒ«", id="change-model")
                yield Button("ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«æˆ»ã‚‹", id="back-menu")
                
        yield Footer()
        
    def on_mount(self) -> None:
        """ç”»é¢ãƒã‚¦ãƒ³ãƒˆæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.load_model()
        
    @work(thread=True)
    def load_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            model_path = self.app.selected_model_path
            self.model, self.tokenizer, self.config = load_model_safetensors(model_path)
            
            # UIã‚’æ›´æ–°
            self.app.call_from_thread(self.update_model_info)
            
        except Exception as e:
            self.app.call_from_thread(
                self.post_message,
                ProcessError(f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            )
            
    def update_model_info(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’æ›´æ–°"""
        info = self.app.selected_model_info
        
        self.query_one("#model-name").update(f"ãƒ¢ãƒ‡ãƒ«: {os.path.basename(self.app.selected_model_path)}")
        self.query_one("#model-domain").update(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {info.get('category', 'Unknown')}")
        self.query_one("#model-size").update(f"ã‚µã‚¤ã‚º: {info.get('model_size', 'Unknown')}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒªã‚¢ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
        self.query_one("#prompt-area").focus()
        
    def action_generate(self) -> None:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
        self.generate_text()
        
    def action_clear(self) -> None:
        """ã‚¯ãƒªã‚¢ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
        self.query_one("#prompt-area").clear()
        self.query_one("#result-area").clear()
        
    def action_back(self) -> None:
        """æˆ»ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
        self.app.pop_screen()
        
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "generate":
            self.generate_text()
        elif event.button.id == "clear":
            self.action_clear()
        elif event.button.id == "change-model":
            self.app.pop_screen()
        elif event.button.id == "back-menu":
            # å…¨ç”»é¢ã‚’ã‚¯ãƒªã‚¢ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¡¨ç¤º
            while len(self.app.screen_stack) > 1:
                self.app.pop_screen()
            await self.app.push_screen(MainMenuScreen())
            
    def generate_text(self) -> None:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ"""
        if not self.model:
            return
            
        prompt = self.query_one("#prompt-area").text.strip()
        if not prompt:
            return
            
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        try:
            temperature = float(self.query_one("#temperature-input").value)
            temperature = max(0.1, min(2.0, temperature))
        except:
            temperature = 0.8
            
        try:
            max_length = int(self.query_one("#max-length-input").value)
            max_length = max(10, min(500, max_length))
        except:
            max_length = 100
            
        try:
            top_k = int(self.query_one("#top-k-input").value)
            top_k = max(1, min(100, top_k))
        except:
            top_k = 40
            
        # ç”Ÿæˆã‚’å®Ÿè¡Œ
        self.generate_text_worker(prompt, temperature, max_length, top_k)
        
    @work(thread=True)
    def generate_text_worker(self, prompt: str, temperature: float, max_length: int, top_k: int) -> None:
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # çµæœã‚¨ãƒªã‚¢ã‚’ã‚¯ãƒªã‚¢
            self.app.call_from_thread(
                self.query_one("#result-area").clear
            )
            
            # ç”Ÿæˆä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            self.app.call_from_thread(
                self.query_one("#result-area").write,
                "[dim]ç”Ÿæˆä¸­...[/dim]"
            )
            
            # ç”Ÿæˆå®Ÿè¡Œ
            generated = generate_sample(
                self.model, 
                self.tokenizer, 
                self.config,
                prompt=prompt,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k
            )
            
            # çµæœã‚’è¡¨ç¤º
            self.app.call_from_thread(
                self.display_result,
                generated
            )
            
        except Exception as e:
            self.app.call_from_thread(
                self.query_one("#result-area").write,
                f"[red]ã‚¨ãƒ©ãƒ¼: {str(e)}[/red]"
            )
            
    def display_result(self, text: str) -> None:
        """ç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
        result_area = self.query_one("#result-area")
        result_area.clear()
        result_area.write(text)
        
    @on(ProcessError)
    def on_process_error(self, message: ProcessError) -> None:
        """ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†"""
        result_area = self.query_one("#result-area")
        result_area.write(f"[bold red]âŒ {message.error}[/bold red]")


class CategorySelectionScreen(Screen):
    """ã‚«ãƒ†ã‚´ãƒªé¸æŠç”»é¢"""
    
    CSS = """
    CategorySelectionScreen {
        align: center middle;
    }
    
    #category-container {
        width: 80;
        height: auto;
        border: solid $primary;
        padding: 2;
    }
    
    .category-card {
        margin: 1;
        padding: 1;
        border: solid $surface;
    }
    
    .category-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .category-title {
        text-style: bold;
        color: $primary;
    }
    
    .category-subtitle {
        color: $text-muted;
    }
    
    .category-description {
        margin-top: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    
    Button {
        margin: 0 1;
    }
    """
    
    def compose(self) -> ComposeResult:
        yield Header()
        
        with Container(id="category-container"):
            yield Label("ã©ã®ã‚¿ã‚¤ãƒ—ã®GPTã‚’ä½œæˆã—ã¾ã™ã‹ï¼Ÿ", classes="title")
            
            for key, data in CATEGORIES.items():
                card = CategoryCard(key, data, classes="category-card")
                card.can_focus = True
                yield card
                
        with Horizontal(id="button-container"):
            yield Button("ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›", id="custom", variant="primary")
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        self.query_one(".category-card").focus()
        
    async def on_category_card_clicked(self, message: CategoryCardClicked) -> None:
        """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ¼ãƒ‰ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        self.app.selected_category = message.category_key
        await self.app.push_screen(PresetSelectionScreen(message.category_key))
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "custom":
            await self.app.push_screen(CustomPromptScreen())
        elif event.button.id == "back":
            self.app.pop_screen()


class PresetSelectionScreen(Screen):
    """ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠç”»é¢"""
    
    CSS = """
    PresetSelectionScreen {
        align: center middle;
    }
    
    #preset-container {
        width: 90;
        height: auto;
        max-height: 80%;
        border: solid $primary;
        padding: 2;
    }
    
    .preset-card {
        margin: 1;
        padding: 1;
        border: solid $surface;
    }
    
    .preset-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .preset-title {
        text-style: bold;
        color: $primary;
    }
    
    .preset-prompt {
        color: $text-muted;
        margin-top: 1;
    }
    
    .preset-description {
        margin-top: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    """
    
    def __init__(self, category_key: str):
        super().__init__()
        self.category_key = category_key
        self.category = CATEGORIES[category_key]
        
    def compose(self) -> ComposeResult:
        yield Header()
        
        with ScrollableContainer(id="preset-container"):
            yield Label(f"{self.category['name']} - ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ", classes="title")
            
            for i, preset in enumerate(self.category['presets'], 1):
                card = PresetCard(preset, i, classes="preset-card")
                card.can_focus = True
                yield card
                
        with Horizontal(id="button-container"):
            yield Button("ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›", id="custom", variant="primary")
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        self.query_one(".preset-card").focus()
        
    async def on_preset_card_clicked(self, message: PresetCardClicked) -> None:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        self.app.selected_prompt = message.preset_data['prompt']
        self.app.selected_preset_info = message.preset_data
        await self.app.push_screen(EpochSelectionScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "custom":
            await self.app.push_screen(CustomPromptScreen())
        elif event.button.id == "back":
            self.app.pop_screen()


class EpochSelectionScreen(Screen):
    """ã‚¨ãƒãƒƒã‚¯æ•°é¸æŠç”»é¢"""
    
    CSS = """
    EpochSelectionScreen {
        align: center middle;
    }
    
    #epoch-container {
        width: 80;
        height: auto;
        border: solid $primary;
        padding: 2;
    }
    
    .epoch-option {
        margin: 1;
        padding: 1;
        border: solid $surface;
    }
    
    .epoch-option:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .epoch-title {
        text-style: bold;
        color: $primary;
    }
    
    .epoch-description {
        margin-top: 1;
    }
    
    .epoch-time {
        color: $warning;
        margin-top: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    """
    
    EPOCH_OPTIONS = [
        {
            "epochs": 20,
            "name": "é«˜é€Ÿãƒ†ã‚¹ãƒˆ",
            "description": "å‹•ä½œç¢ºèªç”¨ã€‚æœ€å°é™ã®å­¦ç¿’ã§ç´ æ—©ãçµæœã‚’ç¢ºèª",
            "time_estimate": "GPU: ç´„30ç§’ / CPU: ç´„2-3åˆ†"
        },
        {
            "epochs": 50,
            "name": "æ¨™æº–",
            "description": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸè¨­å®šã€‚åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’",
            "time_estimate": "GPU: ç´„1-2åˆ† / CPU: ç´„5-10åˆ†"
        },
        {
            "epochs": 100,
            "name": "å“è³ªé‡è¦–",
            "description": "ã‚ˆã‚Šæ·±ã„å­¦ç¿’ã€‚ç”Ÿæˆå“è³ªã®å‘ä¸Šã‚’æœŸå¾…",
            "time_estimate": "GPU: ç´„2-5åˆ† / CPU: ç´„15-20åˆ†"
        },
        {
            "epochs": 200,
            "name": "é«˜å“è³ª",
            "description": "ååˆ†ãªå­¦ç¿’æ™‚é–“ã€‚å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚è‰¯å¥½ãªçµæœ",
            "time_estimate": "GPU: ç´„5-10åˆ† / CPU: ç´„30-40åˆ†"
        },
        {
            "epochs": 500,
            "name": "æœ€å¤§å“è³ª",
            "description": "å¾¹åº•çš„ãªå­¦ç¿’ã€‚éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚ã‚Š",
            "time_estimate": "GPU: ç´„15-20åˆ† / CPU: ç´„1-2æ™‚é–“"
        }
    ]
    
    def compose(self) -> ComposeResult:
        yield Header()
        
        with Container(id="epoch-container"):
            yield Label("å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„", classes="title")
            
            for option in self.EPOCH_OPTIONS:
                card = EpochOption(
                    option['epochs'],
                    option['name'],
                    option['description'],
                    option['time_estimate'],
                    classes="epoch-option"
                )
                card.can_focus = True
                yield card
                
        with Horizontal(id="button-container"):
            yield Button("ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›", id="custom", variant="primary")
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        self.query_one(".epoch-option").focus()
        
    async def on_epoch_option_clicked(self, message: EpochOptionClicked) -> None:
        """ã‚¨ãƒãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        self.app.selected_epochs = message.epochs
        await self.app.push_screen(DataGenerationMethodScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "custom":
            await self.app.push_screen(CustomEpochScreen())
        elif event.button.id == "back":
            self.app.pop_screen()


class DataGenerationMethodScreen(Screen):
    """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•é¸æŠç”»é¢"""
    
    CSS = """
    DataGenerationMethodScreen {
        align: center middle;
    }
    
    #method-container {
        width: 90;
        max-height: 80%;
        border: solid $primary;
        padding: 2;
    }
    
    .method-card {
        margin: 2;
        padding: 2;
        border: solid $surface;
    }
    
    .method-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .method-title {
        text-style: bold;
        color: $primary;
    }
    
    .method-description {
        margin-top: 1;
        color: $text;
    }
    
    .advantages-list {
        margin-top: 1;
        margin-left: 2;
    }
    
    .method-advantage {
        color: $success;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    """
    
    def compose(self) -> ComposeResult:
        yield Header()
        
        with ScrollableContainer(id="method-container"):
            yield Label("ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„", classes="title")
            
            template_card = GenerationMethodCard(
                "template",
                "ğŸ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ç”Ÿæˆ",
                "äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ",
                [
                    "ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œ - ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šä¸è¦",
                    "é«˜é€Ÿç”Ÿæˆ - å³åº§ã«çµæœã‚’å–å¾—",
                    "å®‰å®šå‹•ä½œ - å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã«ä¾å­˜ã—ãªã„",
                    "è»½é‡ - ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãŒå°‘ãªã„"
                ],
                classes="method-card"
            )
            template_card.can_focus = True
            yield template_card
            
            ollama_card = GenerationMethodCard(
                "ollama",
                "ğŸ¤– Ollamaãƒ™ãƒ¼ã‚¹ç”Ÿæˆï¼ˆæ¨å¥¨ï¼‰",
                "Ollamaã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ",
                [
                    "é«˜å“è³ª - ã‚ˆã‚Šè‡ªç„¶ã§å¤šæ§˜ãªæ–‡ç« ç”Ÿæˆ",
                    "ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ - ãƒ¢ãƒ‡ãƒ«é¸æŠã«ã‚ˆã‚Šç‰¹æ€§ã‚’èª¿æ•´",
                    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿ å®Ÿ - æŒ‡å®šã—ãŸæ€§è³ªã‚’ã‚ˆã‚Šæ­£ç¢ºã«åæ˜ ",
                    "ç²¾é¸æ©Ÿèƒ½ - ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡ã‚‚å¯èƒ½"
                ],
                classes="method-card"
            )
            ollama_card.can_focus = True
            yield ollama_card
                
        with Horizontal(id="button-container"):
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        self.query_one(".method-card").focus()
        
    async def on_generation_method_clicked(self, message: GenerationMethodClicked) -> None:
        """ç”Ÿæˆæ–¹æ³•ãŒé¸æŠã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        self.app.selected_generation_method = message.method
        
        if message.method == "ollama":
            # Ollamaãƒ¢ãƒ‡ãƒ«é¸æŠç”»é¢ã¸
            await self.app.push_screen(OllamaModelSelectionScreen("generation"))
        else:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®å ´åˆã¯ç›´æ¥ãƒ¡ã‚¤ãƒ³å‡¦ç†ã¸
            await self.app.push_screen(MainProcessScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "back":
            self.app.pop_screen()


class OllamaModelSelectionScreen(Screen):
    """Ollamaãƒ¢ãƒ‡ãƒ«é¸æŠç”»é¢"""
    
    CSS = """
    OllamaModelSelectionScreen {
        align: center middle;
    }
    
    #model-container {
        width: 100;
        height: 80%;
        border: solid $primary;
        padding: 2;
    }
    
    .model-card {
        margin: 1;
        padding: 1;
        border: solid $surface;
    }
    
    .model-card:hover {
        border: solid $primary;
        background: $boost;
    }
    
    .model-card.unavailable {
        opacity: 0.6;
    }
    
    .model-card.unavailable:hover {
        border: solid $surface;
        background: $background;
    }
    
    .model-title {
        text-style: bold;
    }
    
    .model-description {
        margin-top: 1;
    }
    
    .model-install-hint {
        color: $warning;
        margin-top: 1;
    }
    
    #button-container {
        dock: bottom;
        height: 3;
        align: center middle;
    }
    
    #loading {
        align: center middle;
    }
    """
    
    def __init__(self, purpose: str):
        super().__init__()
        self.purpose = purpose  # "generation" or "refinement"
        self.available_models = []
        self.checking_models = True
        
    def compose(self) -> ComposeResult:
        yield Header()
        
        with Container(id="model-container"):
            if self.purpose == "generation":
                yield Label("ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", classes="title")
            else:
                yield Label("ãƒ‡ãƒ¼ã‚¿ç²¾é¸ï¼ˆå“è³ªè©•ä¾¡ï¼‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", classes="title")
            
            with ScrollableContainer(id="model-list"):
                yield LoadingIndicator(id="loading")
                
        with Horizontal(id="button-container"):
            if self.purpose == "refinement":
                yield Button("ç”Ÿæˆã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨", id="same-model", variant="primary")
            yield Button("æˆ»ã‚‹", id="back")
            
        yield Footer()
        
    def on_mount(self) -> None:
        """ç”»é¢ãƒã‚¦ãƒ³ãƒˆæ™‚ã«Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯"""
        self.check_ollama_models()
        
    @work(thread=True)
    def check_ollama_models(self) -> None:
        """åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(f"{self.app.args.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json()
                self.available_models = [m['name'] for m in models.get('models', [])]
            else:
                self.available_models = []
        except:
            self.available_models = []
        
        self.app.call_from_thread(self.display_models)
        
    def display_models(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’è¡¨ç¤º"""
        self.checking_models = False
        
        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã‚’å‰Šé™¤
        try:
            self.query_one("#loading").remove()
        except:
            pass
        
        model_list = self.query_one("#model-list")
        
        # ã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
        category = self.app.selected_category or "general"
        
        # ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ
        for model_name, model_info in OLLAMA_MODEL_INFO.items():
            available = model_name in self.available_models
            recommended = category in model_info.get('best_for', [])
            
            card = ModelCardOllama(
                model_name,
                model_info,
                available,
                recommended,
                self.purpose,
                classes="model-card" + (" unavailable" if not available else "")
            )
            if available:
                card.can_focus = True
            model_list.mount(card)
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã®è­¦å‘Š
        if not self.available_models:
            model_list.mount(Label(
                "[red]Ollamaãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚[/red]\n"
                "OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚\n"
                "ä¾‹: ollama pull llama3",
                classes="warning"
            ))
        
    async def on_model_card_ollama_clicked(self, message: ModelCardOllamaClicked) -> None:
        """ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚ŒãŸæ™‚ã®å‡¦ç†"""
        if self.purpose == "generation":
            self.app.selected_generation_model = message.model_name
            # ç²¾é¸ç”¨ãƒ¢ãƒ‡ãƒ«é¸æŠã¸
            await self.app.push_screen(OllamaModelSelectionScreen("refinement"))
        else:
            self.app.selected_refinement_model = message.model_name
            # ãƒ¡ã‚¤ãƒ³å‡¦ç†ç”»é¢ã¸
            await self.app.push_screen(MainProcessScreen())
            
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "same-model":
            # ç”Ÿæˆã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ç²¾é¸ã«ã‚‚ä½¿ç”¨
            self.app.selected_refinement_model = self.app.selected_generation_model
            await self.app.push_screen(MainProcessScreen())
        elif event.button.id == "back":
            self.app.pop_screen()


class CustomPromptScreen(ModalScreen):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ç”»é¢"""
    
    CSS = """
    CustomPromptScreen {
        align: center middle;
    }
    
    #dialog {
        width: 60;
        height: auto;
        border: solid $primary;
        background: $surface;
        padding: 2;
    }
    
    Input {
        margin: 1 0;
    }
    
    #button-container {
        margin-top: 2;
        align: center middle;
    }
    """
    
    def compose(self) -> ComposeResult:
        with Container(id="dialog"):
            yield Label("ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", classes="title")
            yield Input(placeholder="ä¾‹: æ–™ç†ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹æ¥½ã—ã„GPT", id="prompt-input")
            
            with Horizontal(id="button-container"):
                yield Button("æ±ºå®š", id="ok", variant="primary")
                yield Button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", id="cancel")
                
    def on_mount(self) -> None:
        self.query_one("#prompt-input").focus()
        
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "ok":
            prompt = self.query_one("#prompt-input").value.strip()
            if prompt:
                self.app.selected_prompt = prompt
                self.app.selected_preset_info = None
                self.dismiss(True)
                # ãƒ¢ãƒ¼ãƒ€ãƒ«çµ‚äº†å¾Œã«ç”»é¢é·ç§»
                await self.app.push_screen(EpochSelectionScreen())
        elif event.button.id == "cancel":
            self.dismiss(False)


class CustomEpochScreen(ModalScreen):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒãƒƒã‚¯æ•°å…¥åŠ›ç”»é¢"""
    
    CSS = """
    CustomEpochScreen {
        align: center middle;
    }
    
    #dialog {
        width: 50;
        height: auto;
        border: solid $primary;
        background: $surface;
        padding: 2;
    }
    
    Input {
        margin: 1 0;
    }
    
    #button-container {
        margin-top: 2;
        align: center middle;
    }
    """
    
    def compose(self) -> ComposeResult:
        with Container(id="dialog"):
            yield Label("ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", classes="title")
            yield Label("(1-1000ã®ç¯„å›²)")
            yield Input(placeholder="ä¾‹: 150", id="epoch-input", type="integer")
            
            with Horizontal(id="button-container"):
                yield Button("æ±ºå®š", id="ok", variant="primary")
                yield Button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", id="cancel")
                
    def on_mount(self) -> None:
        self.query_one("#epoch-input").focus()
        
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        if event.button.id == "ok":
            try:
                epochs = int(self.query_one("#epoch-input").value)
                if 1 <= epochs <= 1000:
                    self.app.selected_epochs = epochs
                    self.dismiss(True)
                    await self.app.push_screen(DataGenerationMethodScreen())
            except ValueError:
                pass
        elif event.button.id == "cancel":
            self.dismiss(False)


class MainProcessScreen(Screen):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ç”»é¢"""
    
    BINDINGS = [
        Binding("space", "toggle_pause", "ä¸€æ™‚åœæ­¢/å†é–‹"),
        Binding("q", "quit", "çµ‚äº†"),
        Binding("f", "try_inference", "æ¨è«–ã‚’è©¦ã™", show=False),
    ]
    
    CSS = """
    MainProcessScreen {
        layout: grid;
        grid-size: 2 2;
        grid-rows: 1fr 1fr;
        grid-columns: 1fr 1fr;
    }
    
    #progress-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    #preview-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    #system-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    #status-panel {
        border: solid $primary;
        padding: 1;
        margin: 1;
    }
    
    .step-item {
        margin: 1 0;
    }
    
    .step-complete {
        color: $success;
    }
    
    .step-current {
        color: $warning;
        text-style: bold;
    }
    
    .step-pending {
        color: $text-muted;
    }
    
    ProgressBar {
        margin: 1 0;
    }
    """
    
    paused = reactive(False)
    current_step = reactive(0)
    
    def __init__(self):
        super().__init__()
        self.worker = None
        self.step_progress = {}
        self.system_info = {}
        self.completed_model_path = None
        self.inference_ready = False
        
    def compose(self) -> ComposeResult:
        yield Header()
        
        # Progress Panel
        with Container(id="progress-panel"):
            yield Label("ğŸ“Š Progress", classes="panel-title")
            yield Static(id="steps-list")
            
        # Preview Panel  
        with Container(id="preview-panel"):
            yield Label("ğŸ‘ï¸ Live Preview", classes="panel-title")
            yield RichLog(highlight=True, markup=True, id="preview-log")
            
        # System Info Panel
        with Container(id="system-panel"):
            yield Label("ğŸ–¥ï¸ System Info", classes="panel-title")
            yield Static(id="system-info")
            
        # Status Panel
        with Container(id="status-panel"):
            yield Label("ğŸ“‹ Status", classes="panel-title")
            yield Static(id="status-info")
            
        yield Footer()
        
    def on_mount(self) -> None:
        """ç”»é¢ãƒã‚¦ãƒ³ãƒˆæ™‚ã®å‡¦ç†"""
        # åˆæœŸçŠ¶æ…‹ã‚’è¡¨ç¤º
        self.update_status_info()
        self.initialize_progress_display()
        self.initialize_system_info()
        self.initialize_preview()
        
        # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹
        self.start_main_process()
        
    def initialize_progress_display(self) -> None:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã®åˆæœŸåŒ–"""
        steps_widget = self.query_one("#steps-list")
        
        steps = [
            ("Data Generation", "data_gen"),
            ("Data Refinement", "data_refine"),
            ("Dataset Creation", "dataset"),
            ("Model Initialization", "model_init"),
            ("Model Training", "training"),
            ("Generation Test", "test"),
            ("Model & Dataset Saving", "save")
        ]
        
        content = []
        for i, (name, key) in enumerate(steps):
            style = "step-pending"
            status = " "
            content.append(f"[{style}][{status}] {i+1}. {name}[/{style}]")
        
        steps_widget.update("\n".join(content))
        
    def initialize_system_info(self) -> None:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®åˆæœŸåŒ–"""
        system_widget = self.query_one("#system-info")
        
        content = []
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            content.append(f"[bold]GPU:[/bold] {device_name}")
            content.append(f"[bold]Memory:[/bold] 0.0/{memory_total:.1f} GB")
        else:
            content.append("[bold]Device:[/bold] CPU")
        
        content.append("[bold]Status:[/bold] Initializing...")
        system_widget.update("\n".join(content))
        
    def initialize_preview(self) -> None:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®åˆæœŸåŒ–"""
        preview_log = self.query_one("#preview-log", RichLog)
        preview_log.write("[dim]Waiting for generation to start...[/dim]")
        
    def update_status_info(self) -> None:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’æ›´æ–°"""
        status_widget = self.query_one("#status-info")
        
        prompt = self.app.selected_prompt[:50] + "..." if len(self.app.selected_prompt) > 50 else self.app.selected_prompt
        epochs = self.app.selected_epochs if hasattr(self.app, 'selected_epochs') else 20
        
        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•ã®æƒ…å ±ã‚’è¿½åŠ 
        gen_method = getattr(self.app, 'selected_generation_method', 'template')
        if gen_method == "ollama":
            gen_model = getattr(self.app, 'selected_generation_model', 'llama3')
            ref_model = getattr(self.app, 'selected_refinement_model', gen_model)
            gen_info = f"Ollama ({gen_model} / {ref_model})"
        else:
            gen_info = "Template-based"
        
        status_text = f"""
[bold]Prompt:[/bold] {prompt}
[bold]Epochs:[/bold] {epochs}
[bold]Model Size:[/bold] {self.app.args.model_size}
[bold]Generation:[/bold] {gen_info}
[bold]Device:[/bold] {'CUDA' if torch.cuda.is_available() else 'CPU'}
[bold]Status:[/bold] {'â¸ï¸ PAUSED' if self.paused else 'â–¶ï¸ Running'}
        """
        
        status_widget.update(status_text)
        
    def action_toggle_pause(self) -> None:
        """ä¸€æ™‚åœæ­¢/å†é–‹"""
        self.paused = not self.paused
        self.update_status_info()
        
    def action_quit(self) -> None:
        """çµ‚äº†"""
        if self.worker:
            self.worker.cancel()
        self.app.exit()
        
    @work(thread=True)
    def start_main_process(self) -> None:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹"""
        try:
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
            self.app.call_from_thread(
                self.post_message,
                PreviewUpdate("Starting main process...")
            )
            
            # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’å–å¾—
            app = self.app
            config = app.config
            args = app.args
            tokenizer = app.tokenizer
            user_prompt = app.selected_prompt
            preset_info = app.selected_preset_info
            
            # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¨­å®š
            if hasattr(app, 'selected_epochs'):
                config.num_epochs = app.selected_epochs
            
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ–¹æ³•ã®è¨­å®šã‚’å–å¾—
            generation_method = getattr(app, 'selected_generation_method', 'template')
            generation_model = getattr(app, 'selected_generation_model', 'llama3')
            refinement_model = getattr(app, 'selected_refinement_model', generation_model)
            
            # TextualCallbacksã‚’ä½œæˆ
            callbacks = TextualCallbacks(self)
            
            # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œ
            folder_path, domain = run_main_process(
                config, args, user_prompt, tokenizer, preset_info, callbacks,
                generation_method=generation_method,
                generation_model=generation_model,
                refinement_model=refinement_model
            )
            
            # å®Œäº†ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’ä¿å­˜
            self.completed_model_path = folder_path
            
            # å®Œäº†é€šçŸ¥
            self.app.call_from_thread(
                self.post_message,
                ProcessComplete(folder_path, domain)
            )
            
        except Exception as e:
            self.app.call_from_thread(
                self.post_message,
                ProcessError(str(e))
            )
            
    @on(ProgressUpdate)
    def on_progress_update(self, message: ProgressUpdate) -> None:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ã‚’å‡¦ç†"""
        self.step_progress[message.step_key] = {
            'current': message.current,
            'total': message.total,
            'info': message.info
        }
        self.update_progress_display()
        
    @on(PreviewUpdate)
    def on_preview_update(self, message: PreviewUpdate) -> None:
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°ã‚’å‡¦ç†"""
        preview_log = self.query_one("#preview-log", RichLog)
        
        if isinstance(message.content, dict):
            for key, value in message.content.items():
                preview_log.write(f"[bold]{key}:[/bold] {value}")
        else:
            preview_log.write(str(message.content))
            
    @on(SystemInfoUpdate)
    def on_system_info_update(self, message: SystemInfoUpdate) -> None:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°ã‚’å‡¦ç†"""
        self.system_info[message.key] = message.value
        self.update_system_info_display()
        
    @on(StepChange)
    def on_step_change(self, message: StepChange) -> None:
        """ã‚¹ãƒ†ãƒƒãƒ—å¤‰æ›´ã‚’å‡¦ç†"""
        self.current_step = message.step
        self.update_progress_display()
        
    def update_progress_display(self) -> None:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã‚’æ›´æ–°"""
        steps_widget = self.query_one("#steps-list")
        
        steps = [
            ("Data Generation", "data_gen"),
            ("Data Refinement", "data_refine"),
            ("Dataset Creation", "dataset"),
            ("Model Initialization", "model_init"),
            ("Model Training", "training"),
            ("Generation Test", "test"),
            ("Model & Dataset Saving", "save")
        ]
        
        content = []
        for i, (name, key) in enumerate(steps):
            if i < self.current_step:
                status = "âœ“"
                style = "step-complete"
            elif i == self.current_step:
                status = "â–º"
                style = "step-current"
            else:
                status = " "
                style = "step-pending"
                
            content.append(f"[{style}][{status}] {i+1}. {name}[/{style}]")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
            if key in self.step_progress and i <= self.current_step:
                progress = self.step_progress[key]
                if progress['current'] is not None and progress['total'] is not None:
                    percentage = progress['current'] / progress['total'] if progress['total'] > 0 else 0
                    bar_width = 30
                    filled = int(bar_width * percentage)
                    bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
                    content.append(f"    {bar} {progress['current']}/{progress['total']}")
                    
                if progress['info']:
                    content.append(f"    {progress['info']}")
                    
        steps_widget.update("\n".join(content))
        
    def update_system_info_display(self) -> None:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤ºã‚’æ›´æ–°"""
        system_widget = self.query_one("#system-info")
        
        content = []
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            memory_used = torch.cuda.memory_allocated(0) / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            content.append(f"[bold]GPU:[/bold] {device_name}")
            content.append(f"[bold]Memory:[/bold] {memory_used:.1f}/{memory_total:.1f} GB")
        else:
            content.append("[bold]Device:[/bold] CPU")
            
        for key, value in self.system_info.items():
            content.append(f"[bold]{key}:[/bold] {value}")
            
        system_widget.update("\n".join(content))
        
    @on(ProcessComplete)
    def on_process_complete(self, message: ProcessComplete) -> None:
        """å‡¦ç†å®Œäº†ã‚’å‡¦ç†"""
        preview_log = self.query_one("#preview-log", RichLog)
        preview_log.write("\n" + "="*50)
        preview_log.write("[bold green]âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼[/bold green]")
        preview_log.write(f"[bold]ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€:[/bold] {message.folder_path}")
        preview_log.write(f"[bold]ãƒ‰ãƒ¡ã‚¤ãƒ³:[/bold] {message.domain}")
        preview_log.write("="*50)
        
        # æ¨è«–ã‚’è©¦ã™ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        if self.completed_model_path:
            preview_log.write("\n[bold cyan]ã“ã®ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’è©¦ã—ã¾ã™ã‹ï¼Ÿ[/bold cyan]")
            preview_log.write("Fã‚­ãƒ¼ã‚’æŠ¼ã™ã¨æ¨è«–ç”»é¢ã¸ç§»å‹•ã—ã¾ã™")
            
            # æ¨è«–æº–å‚™ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
            self.inference_ready = True
            
    def action_try_inference(self) -> None:
        """ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã‚’è©¦ã™"""
        if not self.inference_ready or not self.completed_model_path:
            return
            
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        info_path = os.path.join(self.completed_model_path, "generation_info.json")
        with open(info_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«è¨­å®š
        self.app.selected_model_path = self.completed_model_path
        self.app.selected_model_info = model_info
        
        # å…¨ç”»é¢ã‚’ãƒãƒƒãƒ—ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«æˆ»ã‚‹
        while len(self.app.screen_stack) > 1:
            self.app.pop_screen()
        
        # æ¨è«–ç”»é¢ã¸
        self.app.push_screen(InferenceScreen())
        
    @on(ProcessError)
    def on_process_error(self, message: ProcessError) -> None:
        """ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†"""
        preview_log = self.query_one("#preview-log", RichLog)
        preview_log.write(f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼: {message.error}[/bold red]")


# ===== 5. Textualå¯¾å¿œã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ =====
class TextualCallbacks:
    """Textualç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…"""
    
    def __init__(self, screen):
        self.screen = screen
        self.app = screen.app if screen else None
        self.worker = None
        
        # Textualãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å–å¾—
        if screen is not None:
            try:
                self.worker = get_current_worker()
            except:
                # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®å ´åˆã¯Noneã®ã¾ã¾
                self.worker = None
        
    def update_progress(self, step_key: str, current: Optional[int] = None, 
                       total: Optional[int] = None, info: Optional[str] = None):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã®æ›´æ–°"""
        if self.screen and self.app and self.worker and not self.worker.is_cancelled:
            self.app.call_from_thread(
                self.screen.post_message,
                ProgressUpdate(step_key, current, total, info)
            )
            
    def update_preview(self, content: Any):
        """ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ›´æ–°"""
        if self.screen and self.app and self.worker and not self.worker.is_cancelled:
            self.app.call_from_thread(
                self.screen.post_message,
                PreviewUpdate(content)
            )
            
    def update_system_info(self, key: str, value: str):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®æ›´æ–°"""
        if self.screen and self.app and self.worker and not self.worker.is_cancelled:
            self.app.call_from_thread(
                self.screen.post_message,
                SystemInfoUpdate(key, value)
            )
            
    def set_current_step(self, step: int):
        """ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨­å®š"""
        if self.screen and self.app and self.worker and not self.worker.is_cancelled:
            self.app.call_from_thread(
                self.screen.post_message,
                StepChange(step)
            )
            
    def wait_if_paused(self):
        """ä¸€æ™‚åœæ­¢ä¸­ã¯å¾…æ©Ÿ"""
        if self.screen and hasattr(self.screen, 'paused'):
            while self.screen.paused:
                time.sleep(0.1)


# ===== 6. Ollamaçµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« =====
class OllamaDataGenerator:
    """Ollamaã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 prompt: str, 
                 tokenizer,
                 ollama_host: str = "http://localhost:11434",
                 model: str = "llama3",
                 use_ollama: bool = True,
                 callbacks: Optional[TextualCallbacks] = None):
        """
        Args:
            prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§è³ª
            tokenizer: ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
            ollama_host: Ollamaã‚µãƒ¼ãƒãƒ¼ã®URL
            model: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«
            use_ollama: Ollamaã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
            callbacks: Textualæ›´æ–°ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        self.prompt = prompt
        self.tokenizer = tokenizer
        self.ollama_host = ollama_host
        self.model = model
        self.use_ollama = use_ollama
        self.callbacks = callbacks
        self.domain = self._extract_domain(prompt)
        
        if self.use_ollama:
            self.ollama_available = self._check_ollama_connection()
        else:
            self.ollama_available = False
            if self.callbacks:
                self.callbacks.update_system_info("Generation", "Template-based")
        
    def _check_ollama_connection(self) -> bool:
        """Ollamaã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã‚’ç¢ºèª"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json()
                model_names = [m['name'] for m in models.get('models', [])]
                
                # ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º
                model_info = OLLAMA_MODEL_INFO.get(self.model, {})
                model_display = f"{self.model} ({model_info.get('size', 'Unknown')})"
                
                if self.model in model_names:
                    status = f"âœ“ {model_display}"
                else:
                    status = f"Ã— {self.model} not found"
                
                if self.callbacks:
                    self.callbacks.update_system_info("Generation Model", status)
                
                print(f"Ollamaæ¥ç¶šæˆåŠŸã€‚åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {model_names}")
                
                if self.model not in model_names:
                    print(f"è­¦å‘Š: {self.model}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    if model_names:
                        self.model = model_names[0]
                        print(f"ä»£ã‚ã‚Šã«{self.model}ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                        return True
                    return False
                return True
            return False
        except:
            if self.callbacks:
                self.callbacks.update_system_info("Generation", "Ã— Offline")
            return False
    
    def _extract_domain(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        for cat_key, cat_data in CATEGORIES.items():
            if cat_data['name_ja'] in prompt:
                return cat_key
            for preset in cat_data['presets']:
                if any(keyword in prompt for keyword in preset['prompt'].split()):
                    return cat_key
                    
        if "æ–™ç†" in prompt or "ãƒ¬ã‚·ãƒ”" in prompt:
            return "cooking"
        elif "è©©" in prompt or "ãƒã‚¨ãƒ " in prompt or "å‰µä½œ" in prompt:
            return "poetry"
        elif "æŠ€è¡“" in prompt or "ãƒ—ãƒ­ã‚°ãƒ©" in prompt:
            return "technical"
        else:
            return "general"
    
    def generate_with_ollama(self, prompt: str, max_tokens: int = 100) -> str:
        """Ollamaã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": 0.8,
                        "top_p": 0.9,
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['response'].strip()
            else:
                return self._fallback_generation()
                
        except Exception as e:
            return self._fallback_generation()
    
    def _fallback_generation(self) -> str:
        """OllamaãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰"""
        if self.domain == "cooking":
            return self._generate_cooking_sample()
        elif self.domain == "poetry":
            return self._generate_poetry_sample()
        elif self.domain == "technical":
            return self._generate_technical_sample()
        else:
            return self._generate_general_sample()
    
    def _generate_cooking_sample(self) -> str:
        """æ–™ç†ãƒ¬ã‚·ãƒ”ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰"""
        dishes = ["ãƒ‘ã‚¹ã‚¿", "ã‚«ãƒ¬ãƒ¼", "ã‚µãƒ©ãƒ€", "ã‚¹ãƒ¼ãƒ—", "ã‚±ãƒ¼ã‚­", "å¯¿å¸", "å¤©ã·ã‚‰", "ãƒ©ãƒ¼ãƒ¡ãƒ³"]
        ingredients = ["ãƒˆãƒãƒˆ", "ç‰ã­ã", "ã«ã‚“ã˜ã‚“", "ã˜ã‚ƒãŒã„ã‚‚", "è‚‰", "é­š", "åµ", "ãƒãƒ¼ã‚º"]
        
        dish = random.choice(dishes)
        ing1, ing2 = random.sample(ingredients, 2)
        
        templates = [
            f"{dish}ã®ä½œã‚Šæ–¹ï¼šã¾ãš{ing1}ã‚’åˆ‡ã‚Šã¾ã™ã€‚æ¬¡ã«{ing2}ã‚’åŠ ãˆã¦ç‚’ã‚ã¾ã™ã€‚",
            f"ç°¡å˜{dish}ãƒ¬ã‚·ãƒ”ã€‚{ing1}ã¨{ing2}ã‚’ä½¿ã£ãŸç¾å‘³ã—ã„æ–™ç†ã§ã™ã€‚",
            f"ä»Šæ—¥ã®çŒ®ç«‹ã¯{dish}ã€‚ææ–™ã¯{ing1}ã€{ing2}ãªã©ã§ã™ã€‚",
            f"ãƒ—ãƒ­ãŒæ•™ãˆã‚‹{dish}ã®æ¥µæ„ã€‚{ing1}ã®ä¸‹å‡¦ç†ãŒãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚"
        ]
        
        return random.choice(templates)
    
    def _generate_poetry_sample(self) -> str:
        """è©©çš„ãªã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰"""
        themes = ["æ˜¥", "å¤", "ç§‹", "å†¬", "æ„›", "å¸Œæœ›", "å¤¢", "æ™‚é–“", "è¨˜æ†¶"]
        emotions = ["å–œã³", "æ‚²ã—ã¿", "æ‡ã‹ã—ã•", "æœŸå¾…", "å®‰ã‚‰ã", "åˆ‡ãªã•", "æ„Ÿå‹•"]
        
        theme = random.choice(themes)
        emotion = random.choice(emotions)
        
        templates = [
            f"{theme}ã®{emotion}ã‚’æ„Ÿã˜ãªãŒã‚‰ã€ç§ã¯é™ã‹ã«æ­©ãã€‚",
            f"{emotion}ã«åŒ…ã¾ã‚ŒãŸ{theme}ã®æ—¥ã€…ã€‚å¿ƒã«éŸ¿ãç¬é–“ã€‚",
            f"{theme}ã‚ˆã€æ°¸é ã«ã€‚{emotion}ã¨å…±ã«ç”Ÿãã¦ã„ãã€‚",
            f"è¨€è‘‰ã«ã§ããªã„{emotion}ãŒã€{theme}ã®ä¸­ã§æºã‚Œã¦ã„ã‚‹ã€‚"
        ]
        
        return random.choice(templates)
    
    def _generate_technical_sample(self) -> str:
        """æŠ€è¡“æ–‡æ›¸ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰"""
        topics = ["Python", "æ©Ÿæ¢°å­¦ç¿’", "API", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ã‚¯ãƒ©ã‚¦ãƒ‰"]
        actions = ["å®Ÿè£…", "æœ€é©åŒ–", "è¨­è¨ˆ", "åˆ†æ", "æ§‹ç¯‰", "ãƒ‡ãƒãƒƒã‚°", "ãƒ†ã‚¹ãƒˆ"]
        
        topic = random.choice(topics)
        action = random.choice(actions)
        
        templates = [
            f"{topic}ã®{action}ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ã¾ãšåŸºæœ¬çš„ãªæ¦‚å¿µã‹ã‚‰å§‹ã‚ã¾ã™ã€‚",
            f"{action}ã‚’è¡Œã†éš›ã®{topic}ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚",
            f"åŠ¹ç‡çš„ãª{topic}ã®{action}æ–¹æ³•ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã‚³ãƒ„ã€‚",
            f"{topic}ã«ãŠã‘ã‚‹{action}ã®é‡è¦æ€§ã¨å…·ä½“çš„ãªæ‰‹é †ã‚’è§£èª¬ã—ã¾ã™ã€‚"
        ]
        
        return random.choice(templates)
    
    def _generate_general_sample(self) -> str:
        """ä¸€èˆ¬çš„ãªã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰"""
        subjects = ["ä»Šæ—¥", "æ˜æ—¥", "äººç”Ÿ", "ä¸–ç•Œ", "ç§ãŸã¡", "ç¤¾ä¼š", "æœªæ¥"]
        predicates = ["ç´ æ™´ã‚‰ã—ã„", "æ–°ã—ã„", "å¤§åˆ‡ãª", "èˆˆå‘³æ·±ã„", "æ¥½ã—ã„", "æŒ‘æˆ¦çš„ãª"]
        
        subj = random.choice(subjects)
        pred = random.choice(predicates)
        
        templates = [
            f"{subj}ã¯{pred}ã€‚ãã‚ŒãŒç§ã®è€ƒãˆã§ã™ã€‚",
            f"{pred}ãª{subj}ã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚",
            f"{subj}ã‚’ã‚ˆã‚Š{pred}ã‚‚ã®ã«ã™ã‚‹ãŸã‚ã«ã€ç§ãŸã¡ãŒã§ãã‚‹ã“ã¨ã¯ä½•ã§ã—ã‚‡ã†ã‹ã€‚"
        ]
        
        return random.choice(templates)
    
    def generate_samples(self, num_tokens: int) -> List[str]:
        """æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ"""
        samples = []
        current_tokens = 0
        
        if self.use_ollama and self.ollama_available:
            model_info = OLLAMA_MODEL_INFO.get(self.model, {})
            print(f"\nOllamaã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒ¡ã‚¤ãƒ³ '{self.domain}' ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            print(f"ãƒ¢ãƒ‡ãƒ«: {self.model} - {model_info.get('description', 'No description')}")
            generation_prompts = self._create_generation_prompts()
        else:
            if not self.use_ollama:
                print(f"\nãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆä¸­...")
            else:
                print(f"\nOllamaãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆä¸­...")
            print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {self.domain}")
        
        # Textualç”¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å‡¦ç†ã¨tqdmã®åˆ‡ã‚Šæ›¿ãˆ
        if self.callbacks and self.callbacks.app:
            total_iterations = 0
            prompt_index = 0
            
            while current_tokens < num_tokens:
                self.callbacks.wait_if_paused()
                
                if self.use_ollama and self.ollama_available:
                    current_prompt = generation_prompts[prompt_index % len(generation_prompts)]
                    sample = self.generate_with_ollama(current_prompt, max_tokens=150)
                    time.sleep(0.5)
                else:
                    sample = self._fallback_generation()
                
                if sample and len(sample) > 20:
                    samples.append(sample)
                    sample_tokens = len(self.tokenizer.encode(sample))
                    current_tokens += sample_tokens
                    
                    # Textualæ›´æ–°
                    self.callbacks.update_progress(
                        'data_gen',
                        current=current_tokens,
                        total=num_tokens,
                        info=f"Samples: {len(samples)} | Mode: {'Ollama' if self.use_ollama and self.ollama_available else 'Template'}"
                    )
                    
                    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°
                    if len(samples) % 5 == 0:
                        self.callbacks.update_preview({
                            "Latest Sample": sample[:100] + "...",
                            "Total Samples": len(samples),
                            "Tokens Generated": current_tokens
                        })
                
                prompt_index += 1
                total_iterations += 1
                
                if total_iterations > 100 and current_tokens < num_tokens * 0.5:
                    break
        else:
            # éTextualãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã®tqdmä½¿ç”¨ï¼‰
            pbar = tqdm(total=num_tokens, desc="ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ", unit="tokens")
            prompt_index = 0
            
            while current_tokens < num_tokens:
                if self.use_ollama and self.ollama_available:
                    current_prompt = generation_prompts[prompt_index % len(generation_prompts)]
                    sample = self.generate_with_ollama(current_prompt, max_tokens=150)
                    time.sleep(0.5)
                else:
                    sample = self._fallback_generation()
                
                if sample and len(sample) > 20:
                    samples.append(sample)
                    sample_tokens = len(self.tokenizer.encode(sample))
                    current_tokens += sample_tokens
                    pbar.update(sample_tokens)
                
                prompt_index += 1
                
                if len(samples) > 100 and current_tokens < num_tokens * 0.5:
                    break
            
            pbar.close()
        
        print(f"ç”Ÿæˆå®Œäº†: {len(samples)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã€ç´„{current_tokens}ãƒˆãƒ¼ã‚¯ãƒ³")
        
        return samples
    
    def _create_generation_prompts(self) -> List[str]:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¿œã˜ãŸç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
        base_instruction = f"ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€{self.prompt}ã‚ˆã†ãªæ–‡ç« ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
        
        if self.domain == "cooking":
            return [
                base_instruction + "ç°¡å˜ãªæ–™ç†ãƒ¬ã‚·ãƒ”ã‚’1ã¤æ›¸ã„ã¦ãã ã•ã„ã€‚ææ–™ã¨æ‰‹é †ã‚’å«ã‚ã¦ãã ã•ã„ã€‚",
                base_instruction + "å­£ç¯€ã®é£Ÿæã‚’ä½¿ã£ãŸæ–™ç†ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "åˆå¿ƒè€…å‘ã‘ã®æ–™ç†ã®ã‚³ãƒ„ã‚’1ã¤æ•™ãˆã¦ãã ã•ã„ã€‚",
                base_instruction + "ãƒ—ãƒ­ãŒä½¿ã†èª¿ç†æŠ€è¡“ã‚’1ã¤ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "å¥åº·çš„ãªæ–™ç†ã®ãƒã‚¤ãƒ³ãƒˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
            ]
        elif self.domain == "poetry":
            return [
                base_instruction + "è‡ªç„¶ã‚’ãƒ†ãƒ¼ãƒã«ã—ãŸçŸ­ã„è©©ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                base_instruction + "æ„Ÿæƒ…ã‚’è¡¨ç¾ã™ã‚‹è©©çš„ãªæ–‡ç« ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                base_instruction + "å­£ç¯€ã®ç§»ã‚ã„ã‚’è¡¨ç¾ã™ã‚‹çŸ­æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                base_instruction + "äººç”Ÿã«ã¤ã„ã¦è€ƒå¯Ÿã™ã‚‹è©©çš„ãªæ–‡ç« ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                base_instruction + "æ„›ã‚„å‹æƒ…ã«ã¤ã„ã¦ã®è©©ã‚’å‰µä½œã—ã¦ãã ã•ã„ã€‚"
            ]
        elif self.domain == "technical":
            return [
                base_instruction + "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µã‚’1ã¤èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "æŠ€è¡“çš„ãªå•é¡Œè§£æ±ºã®æ–¹æ³•ã‚’1ã¤ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’1ã¤èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã®é‡è¦ãªåŸå‰‡ã‚’1ã¤è§£èª¬ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "æœ€æ–°ã®æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
            ]
        else:  # general
            return [
                base_instruction + "æ—¥å¸¸çš„ãªè©±é¡Œã«ã¤ã„ã¦è‡ªç„¶ãªæ–‡ç« ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
                base_instruction + "èˆˆå‘³æ·±ã„äº‹å®Ÿã‚„çŸ¥è­˜ã‚’1ã¤å…±æœ‰ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "èº«è¿‘ãªå‡ºæ¥äº‹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                base_instruction + "ç¤¾ä¼šçš„ãªè©±é¡Œã«ã¤ã„ã¦æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚",
                base_instruction + "äººç”Ÿã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’1ã¤æ›¸ã„ã¦ãã ã•ã„ã€‚"
            ]


# ===== 7. ãƒ‡ãƒ¼ã‚¿ç²¾é¸ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« =====
class DataRefiner:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ç²¾é¸ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, prompt: str, tokenizer, use_ollama: bool = False,
                 ollama_host: str = "http://localhost:11434", model: str = "llama3",
                 callbacks: Optional[TextualCallbacks] = None):
        self.prompt = prompt
        self.tokenizer = tokenizer
        self.use_ollama = use_ollama
        self.ollama_host = ollama_host
        self.model = model
        self.callbacks = callbacks
        
        if self.use_ollama and callbacks:
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
            model_info = OLLAMA_MODEL_INFO.get(self.model, {})
            model_display = f"{self.model} ({model_info.get('size', 'Unknown')})"
            callbacks.update_system_info("Refinement Model", model_display)
        
    def refine_samples(self, samples: List[str], target_tokens: int) -> List[str]:
        """ã‚µãƒ³ãƒ—ãƒ«ã‚’è©•ä¾¡ã—ã€ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¾ã§ç²¾é¸"""
        print("\nãƒ‡ãƒ¼ã‚¿ç²¾é¸ä¸­...")
        
        scored_samples = []
        
        # Textualç”¨ã®é€²æ—è¡¨ç¤º
        if self.callbacks and self.callbacks.app:
            for i, sample in enumerate(samples):
                self.callbacks.wait_if_paused()
                
                if self.use_ollama:
                    score = self._evaluate_with_ollama(sample)
                else:
                    score = self._calculate_score(sample)
                    
                scored_samples.append((score, sample))
                
                # é€²æ—æ›´æ–°
                self.callbacks.update_progress(
                    'data_refine',
                    current=i + 1,
                    total=len(samples),
                    info=f"Evaluating quality..."
                )
                
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°ï¼ˆ10ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ï¼‰
                if (i + 1) % 10 == 0:
                    avg_score = sum(s[0] for s in scored_samples) / len(scored_samples)
                    self.callbacks.update_preview({
                        "Samples Evaluated": i + 1,
                        "Average Score": f"{avg_score:.2f}",
                        "Latest Score": f"{score:.2f}"
                    })
        else:
            # éTextualãƒ¢ãƒ¼ãƒ‰
            for sample in tqdm(samples, desc="ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡"):
                if self.use_ollama:
                    score = self._evaluate_with_ollama(sample)
                else:
                    score = self._calculate_score(sample)
                scored_samples.append((score, sample))
        
        # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        scored_samples.sort(reverse=True, key=lambda x: x[0])
        
        # ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«é”ã™ã‚‹ã¾ã§ä¸Šä½ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
        refined_samples = []
        current_tokens = 0
        
        for score, sample in scored_samples:
            sample_tokens = len(self.tokenizer.encode(sample))
            if current_tokens + sample_tokens <= target_tokens:
                refined_samples.append(sample)
                current_tokens += sample_tokens
            
            if current_tokens >= target_tokens * 0.95:
                break
        
        avg_score = sum(s[0] for s in scored_samples[:len(refined_samples)]) / len(refined_samples) if refined_samples else 0
        
        # æœ€çµ‚çµæœã®æ›´æ–°
        if self.callbacks and self.callbacks.app:
            self.callbacks.update_progress(
                'data_refine',
                current=len(refined_samples),
                total=len(refined_samples),
                info=f"Quality: {avg_score:.2f} | Tokens: {current_tokens}"
            )
        
        print(f"ç²¾é¸å®Œäº†: {len(samples)} â†’ {len(refined_samples)} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³æ•°: ç´„{current_tokens}")
        print(f"å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_score:.2f}")
        
        return refined_samples
    
    def _evaluate_with_ollama(self, sample: str) -> float:
        """Ollamaã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’è©•ä¾¡"""
        evaluation_prompt = f"""
ä»¥ä¸‹ã®æ–‡ç« ãŒã€Œ{self.prompt}ã€ã¨ã„ã†ç›®çš„ã«ã©ã‚Œãã‚‰ã„é©ã—ã¦ã„ã‚‹ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
0ã‹ã‚‰10ã®æ•°å€¤ã§ç­”ãˆã¦ãã ã•ã„ï¼ˆ0ãŒæœ€ä½ã€10ãŒæœ€é«˜ï¼‰ã€‚æ•°å€¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

æ–‡ç« ï¼š
{sample}

è©•ä¾¡ï¼ˆ0-10ï¼‰ï¼š"""
        
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": evaluation_prompt,
                    "stream": False,
                    "options": {"num_predict": 10, "temperature": 0.1}
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                score_text = result['response'].strip()
                try:
                    score = float(score_text.split()[0])
                    return min(max(score / 10.0, 0.0), 1.0)
                except:
                    return 0.5
            return 0.5
        except:
            return self._calculate_score(sample)
    
    def _calculate_score(self, sample: str) -> float:
        """ã‚µãƒ³ãƒ—ãƒ«ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰"""
        score = 0.0
        
        # é•·ã•ã‚¹ã‚³ã‚¢
        length = len(sample)
        if 20 <= length <= 100:
            score += 1.0
        elif 10 <= length <= 150:
            score += 0.5
        
        # å¥èª­ç‚¹ã®é©åˆ‡ãªä½¿ç”¨
        if sample.count('ã€‚') >= 1:
            score += 0.5
        if sample.count('ã€') >= 1:
            score += 0.3
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã®é–¢é€£æ€§ï¼ˆç°¡æ˜“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        prompt_words = set(self.prompt.split())
        sample_words = set(sample.split())
        overlap = len(prompt_words & sample_words)
        score += overlap * 0.2
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’å°‘ã—åŠ ãˆã‚‹ï¼ˆå¤šæ§˜æ€§ã®ãŸã‚ï¼‰
        score += random.random() * 0.3
        
        return score


# ===== 8. ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ =====
class TextDataset(Dataset):
    """PyTorchã®Datasetã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, texts: List[str], tokenizer, max_length: int):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        self.tokens = []
        for text in texts:
            encoded = tokenizer.encode(text, max_length=max_length, truncation=True)
            self.tokens.extend(encoded)
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {len(self.tokens)} ãƒˆãƒ¼ã‚¯ãƒ³")
    
    def __len__(self):
        return max(1, len(self.tokens) - self.max_length)
    
    def __getitem__(self, idx):
        chunk = self.tokens[idx:idx + self.max_length + 1]
        
        if len(chunk) < self.max_length + 1:
            chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length + 1 - len(chunk))
        
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        
        return x, y


# ===== 9. GPTãƒ¢ãƒ‡ãƒ«ã®å®šç¾© =====
class CausalSelfAttention(nn.Module):
    """ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è‡ªå·±æ³¨æ„æ©Ÿæ§‹"""
    
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.attn_dropout = nn.Dropout(0.1)
        self.resid_dropout = nn.Dropout(0.1)
        
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        
        self.register_buffer("bias", torch.tril(torch.ones(config.n_positions, config.n_positions))
                                     .view(1, 1, config.n_positions, config.n_positions))
    
    def forward(self, x):
        B, T, C = x.size()
        
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        
        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = torch.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        
        y = self.resid_dropout(self.c_proj(y))
        
        return y


class Block(nn.Module):
    """Transformerãƒ–ãƒ­ãƒƒã‚¯"""
    
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = nn.Sequential(
            nn.Linear(config.n_embd, 4 * config.n_embd),
            nn.GELU(),
            nn.Linear(4 * config.n_embd, config.n_embd),
            nn.Dropout(0.1)
        )
    
    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class GPT(nn.Module):
    """GPTãƒ¢ãƒ‡ãƒ«æœ¬ä½“"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        self.drop = nn.Dropout(0.1)
        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        self.apply(self._init_weights)
        
        n_params = sum(p.numel() for p in self.parameters())
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {n_params/1e6:.2f}M")
    
    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
    
    def forward(self, idx, targets=None):
        b, t = idx.size()
        assert t <= self.config.n_positions, f"ç³»åˆ—é•· {t} ãŒæœ€å¤§å€¤ {self.config.n_positions} ã‚’è¶…ãˆã¦ã„ã¾ã™"
        
        pos = torch.arange(0, t, dtype=torch.long, device=idx.device).unsqueeze(0)
        
        tok_emb = self.wte(idx)
        pos_emb = self.wpe(pos)
        x = self.drop(tok_emb + pos_emb)
        
        for block in self.h:
            x = block(x)
        
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        loss = None
        if targets is not None:
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)), 
                targets.view(-1),
                ignore_index=self.config.vocab_size - 1
            )
        
        return logits, loss
    
    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        for _ in range(max_new_tokens):
            idx_cond = idx if idx.size(1) <= self.config.n_positions else idx[:, -self.config.n_positions:]
            
            logits, _ = self(idx_cond)
            logits = logits[:, -1, :] / temperature
            
            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            
            idx = torch.cat((idx, idx_next), dim=1)
        
        return idx


# ===== 10. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•° =====
def train_model(model, train_loader, config, callbacks: Optional[TextualCallbacks] = None):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
    model.train()
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    print(f"\nãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ï¼ˆ{config.num_epochs}ã‚¨ãƒãƒƒã‚¯ï¼‰")
    
    callbacks = callbacks or TextualCallbacks(None)
    
    # åˆæœŸãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚’è¨­å®šï¼ˆä¿®æ­£ç®‡æ‰€ï¼‰
    if callbacks and callbacks.app:
        callbacks.update_progress(
            'training',
            current=0,
            total=config.num_epochs,
            info="Preparing training..."
        )
    
    # æ›´æ–°é »åº¦ã®è¨­å®š
    update_interval = max(1, config.num_epochs // 20)
    preview_interval = max(5, config.num_epochs // 4)
    
    for epoch in range(config.num_epochs):
        total_loss = 0
        num_batches = 0
        
        if callbacks and callbacks.app:
            # Textualãƒ¢ãƒ¼ãƒ‰
            for batch_idx, (x, y) in enumerate(train_loader):
                callbacks.wait_if_paused()
                
                x, y = x.to(config.device), y.to(config.device)
                
                optimizer.zero_grad()
                logits, loss = model(x, y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å¾Œã®æ›´æ–°
            if (epoch + 1) % update_interval == 0 or epoch == 0 or epoch == config.num_epochs - 1:
                # é€²æ—æ›´æ–°
                callbacks.update_progress(
                    'training',
                    current=epoch + 1,
                    total=config.num_epochs,
                    info=f"Loss: {total_loss/num_batches:.4f}"
                )
                
                # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±æ›´æ–°
                if (epoch + 1) % (update_interval * 2) == 0:
                    if torch.cuda.is_available():
                        callbacks.update_system_info(
                            "GPU Memory",
                            f"{torch.cuda.memory_allocated(0)/1024**3:.1f} GB"
                        )
                
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æ›´æ–°
                if (epoch + 1) % preview_interval == 0 or epoch == config.num_epochs - 1:
                    model.eval()
                    sample_text = generate_sample(model, train_loader.dataset.tokenizer, config)
                    callbacks.update_preview({
                        "Epoch": epoch + 1,
                        "Avg Loss": f"{total_loss/num_batches:.4f}",
                        "Sample": sample_text[:100] + "..."
                    })
                    model.train()
                
        else:
            # éTextualãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã®tqdmä½¿ç”¨ï¼‰
            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.num_epochs}")
            for batch_idx, (x, y) in enumerate(pbar):
                x, y = x.to(config.device), y.to(config.device)
                
                optimizer.zero_grad()
                logits, loss = model(x, y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        
        # å®šæœŸçš„ãªç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«ã®è¡¨ç¤ºï¼ˆéTextualãƒ¢ãƒ¼ãƒ‰ï¼‰
        if not (callbacks and callbacks.app) and (epoch + 1) % update_interval == 0:
            print(f"\nEpoch {epoch+1} - å¹³å‡æå¤±: {avg_loss:.4f}")
            model.eval()
            sample_text = generate_sample(model, train_loader.dataset.tokenizer, config)
            print(f"ç”Ÿæˆã‚µãƒ³ãƒ—ãƒ«: {sample_text}")
            model.train()
    
    print("\nãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")


# ===== 11. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•° =====
def generate_sample(model, tokenizer, config, prompt="", max_length=50, temperature=1.0, top_k=40):
    """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    model.eval()
    
    if prompt:
        tokens = tokenizer.encode(prompt)
        idx = torch.tensor([tokens], dtype=torch.long).to(config.device)
    else:
        idx = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long).to(config.device)
    
    with torch.no_grad():
        generated = model.generate(idx, max_new_tokens=max_length, temperature=temperature, top_k=top_k)
    
    generated_text = tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)
    return generated_text


# ===== 12. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿ =====
def save_model_safetensors(model, tokenizer, config, folder_path, metadata=None):
    """ãƒ¢ãƒ‡ãƒ«ã‚’safetensorså½¢å¼ã§ä¿å­˜ï¼ˆãƒ•ã‚©ãƒ«ãƒ€ç®¡ç†å¯¾å¿œï¼‰"""
    print(f"\nãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­: {folder_path}")
    
    os.makedirs(folder_path, exist_ok=True)
    
    model_path = os.path.join(folder_path, "model.safetensors")
    
    state_dict = model.state_dict()
    
    default_metadata = {
        "model_type": "gpt",
        "vocab_size": str(config.vocab_size),
        "n_embd": str(config.n_embd),
        "n_layer": str(config.n_layer),
        "n_head": str(config.n_head),
        "n_positions": str(config.n_positions),
        "description": "Text2GPT1 Generated Model"
    }
    
    if metadata:
        default_metadata.update(metadata)
    
    save_file(state_dict, model_path, metadata=default_metadata)
    
    tokenizer.save_pretrained(folder_path)
    
    config_path = os.path.join(folder_path, "config.json")
    config_dict = {
        "vocab_size": config.vocab_size,
        "n_embd": config.n_embd,
        "n_layer": config.n_layer,
        "n_head": config.n_head,
        "n_positions": config.n_positions
    }
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
    print(f"è¨­å®šä¿å­˜å®Œäº†: {config_path}")
    
    return model_path


def load_model_safetensors(folder_path):
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    # è¨­å®šã‚’èª­ã¿è¾¼ã¿
    config_path = os.path.join(folder_path, "config.json")
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    # Configã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    config = Config()
    config.vocab_size = config_dict['vocab_size']
    config.n_embd = config_dict['n_embd']
    config.n_layer = config_dict['n_layer']
    config.n_head = config_dict['n_head']
    config.n_positions = config_dict['n_positions']
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = GPT(config).to(config.device)
    
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    model_path = os.path.join(folder_path, "model.safetensors")
    state_dict = load_file(model_path)
    model.load_state_dict(state_dict)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    tokenizer = GPT2Tokenizer.from_pretrained(folder_path)
    
    return model, tokenizer, config


def save_generation_info(folder_path, info_dict):
    """ç”Ÿæˆæƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    info_path = os.path.join(folder_path, "generation_info.json")
    
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info_dict, f, indent=2, ensure_ascii=False)
    
    print(f"ç”Ÿæˆæƒ…å ±ä¿å­˜å®Œäº†: {info_path}")


def save_dataset(folder_path, refined_samples, tokenizer, metadata=None):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’JSONå½¢å¼ã§ä¿å­˜"""
    os.makedirs(folder_path, exist_ok=True)
    
    dataset_path = os.path.join(folder_path, "dataset.json")
    
    samples_with_info = []
    total_tokens = 0
    
    for sample in refined_samples:
        tokens = tokenizer.encode(sample)
        token_count = len(tokens)
        total_tokens += token_count
        
        samples_with_info.append({
            "text": sample,
            "tokens": token_count
        })
    
    dataset_info = {
        "metadata": {
            "total_samples": len(refined_samples),
            "total_tokens": total_tokens,
            "average_tokens_per_sample": total_tokens / len(refined_samples) if refined_samples else 0,
            "creation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "tokenizer": "gpt2"
        },
        "samples": samples_with_info
    }
    
    if metadata:
        dataset_info["metadata"].update(metadata)
    
    with open(dataset_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†: {dataset_path}")
    
    text_path = os.path.join(folder_path, "dataset.txt")
    with open(text_path, 'w', encoding='utf-8') as f:
        for i, sample in enumerate(refined_samples):
            f.write(f"=== Sample {i+1} ===\n")
            f.write(sample)
            f.write("\n\n")
    
    print(f"ãƒ†ã‚­ã‚¹ãƒˆç‰ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†: {text_path}")
    
    return dataset_path


def list_saved_models(output_dir="models"):
    """ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—"""
    models = []
    
    if not os.path.exists(output_dir):
        return models
    
    for folder in os.listdir(output_dir):
        folder_path = os.path.join(output_dir, folder)
        info_path = os.path.join(folder_path, "generation_info.json")
        
        if os.path.isdir(folder_path) and os.path.exists(info_path):
            try:
                with open(info_path, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                models.append({
                    "path": folder_path,
                    "info": info
                })
            except:
                pass
    
    # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
    models.sort(key=lambda x: x['info'].get('creation_date', ''), reverse=True)
    
    return models


# ===== 13. ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def run_main_process(config, args, user_prompt, tokenizer, preset_info=None, 
                    callbacks: Optional[TextualCallbacks] = None,
                    generation_method: str = "template",
                    generation_model: str = "llama3",
                    refinement_model: str = "llama3"):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’Textualã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§å®Ÿè¡Œ"""
    callbacks = callbacks or TextualCallbacks(None)
    start_time = time.time()
    
    data_stats = {
        "samples_generated": 0,
        "samples_refined": 0,
        "initial_tokens": config.initial_tokens,
        "final_tokens": config.final_tokens,
        "generation_method": generation_method,
        "generation_model": generation_model if generation_method == "ollama" else None,
        "refinement_model": refinement_model if generation_method == "ollama" else None
    }
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    callbacks.set_current_step(0)
    print(f"\n[ã‚¹ãƒ†ãƒƒãƒ—1] {config.initial_tokens}ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
    
    use_ollama = (generation_method == "ollama" and not args.no_ollama)
    
    generator = OllamaDataGenerator(
        user_prompt, 
        tokenizer,
        ollama_host=args.ollama_host,
        model=generation_model,
        use_ollama=use_ollama,
        callbacks=callbacks
    )
    
    raw_samples = generator.generate_samples(config.initial_tokens)
    data_stats["samples_generated"] = len(raw_samples)
    print(f"ç”Ÿæˆå®Œäº†: {len(raw_samples)}å€‹ã®ã‚µãƒ³ãƒ—ãƒ«")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ç²¾é¸
    callbacks.set_current_step(1)
    print(f"\n[ã‚¹ãƒ†ãƒƒãƒ—2] {config.final_tokens}ãƒˆãƒ¼ã‚¯ãƒ³ã«ç²¾é¸")
    
    # ç²¾é¸ã§Ollamaã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    use_ollama_refine = use_ollama and generator.ollama_available
    
    refiner = DataRefiner(
        user_prompt, 
        tokenizer,
        use_ollama=use_ollama_refine,
        ollama_host=args.ollama_host,
        model=refinement_model,
        callbacks=callbacks
    )
    refined_samples = refiner.refine_samples(raw_samples, config.final_tokens)
    data_stats["samples_refined"] = len(refined_samples)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    callbacks.set_current_step(2)
    print("\n[ã‚¹ãƒ†ãƒƒãƒ—3] ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ")
    dataset = TextDataset(refined_samples, tokenizer, config.max_length)
    train_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)
    callbacks.update_progress('dataset', current=1, total=1, info="Dataset ready")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    callbacks.set_current_step(3)
    print(f"\n[ã‚¹ãƒ†ãƒƒãƒ—4] GPTãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆ{args.model_size}ï¼‰")
    model = GPT(config).to(config.device)
    callbacks.update_progress('model_init', current=1, total=1, info=f"Model: {args.model_size}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    callbacks.set_current_step(4)
    print("\n[ã‚¹ãƒ†ãƒƒãƒ—5] ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°")
    train_model(model, train_loader, config, callbacks)
    
    # ã‚¹ãƒ†ãƒƒãƒ—6: æœ€çµ‚çš„ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ
    callbacks.set_current_step(5)
    print("\n[ã‚¹ãƒ†ãƒƒãƒ—6] å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    test_results = []
    for i in range(3):
        generated = generate_sample(model, tokenizer, config, prompt="", max_length=30)
        test_results.append(generated)
        print(f"ç”Ÿæˆä¾‹{i+1}: {generated}")
        
        callbacks.update_progress('test', current=i+1, total=3, info="Testing generation")
        callbacks.update_preview({f"Test {i+1}": generated})
    
    # ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    callbacks.set_current_step(6)
    print("\n[ã‚¹ãƒ†ãƒƒãƒ—7] ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜")
    
    folder_name = create_model_folder_name(user_prompt, preset_info)
    folder_path = os.path.join(args.output_dir, folder_name)
    
    dataset_metadata = {
        "prompt": user_prompt,
        "domain": generator.domain,
        "generation_method": generation_method,
        "generation_model": generation_model if generation_method == "ollama" else None,
        "refinement_model": refinement_model if generation_method == "ollama" else None,
        "ollama_used": generator.ollama_available and generation_method == "ollama",
        "quality_threshold": "refined"
    }
    save_dataset(folder_path, refined_samples, tokenizer, dataset_metadata)
    
    callbacks.update_progress('save', current=50, total=100, info="Dataset saved, saving model...")
    
    end_time = time.time()
    generation_time = end_time - start_time
    
    metadata = {
        "prompt": user_prompt,
        "domain": generator.domain,
        "model_size": args.model_size,
        "training_tokens": str(config.final_tokens),
        "creation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "generation_method": generation_method,
        "ollama_used": str(generator.ollama_available and generation_method == "ollama")
    }
    
    save_model_safetensors(model, tokenizer, config, folder_path, metadata)
    
    generation_info = {
        "prompt": user_prompt,
        "category": generator.domain,
        "preset": preset_info['title'] if preset_info else "ã‚«ã‚¹ã‚¿ãƒ ",
        "model_size": args.model_size,
        "training_params": {
            "epochs": config.num_epochs,
            "batch_size": config.batch_size,
            "learning_rate": config.learning_rate
        },
        "data_stats": data_stats,
        "dataset_files": {
            "json": "dataset.json",
            "text": "dataset.txt"
        },
        "creation_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "generation_method": generation_method,
        "generation_model": generation_model if generation_method == "ollama" else None,
        "refinement_model": refinement_model if generation_method == "ollama" else None,
        "ollama_used": generator.ollama_available and generation_method == "ollama",
        "generation_time": f"{generation_time:.1f} seconds"
    }
    
    save_generation_info(folder_path, generation_info)
    
    callbacks.update_progress('save', current=100, total=100, info="All files saved")
    
    return folder_path, generator.domain


# ===== 14. Textualã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ =====
class Text2GPT1App(App):
    """Text2GPT1ã®Textualã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    CSS = """
    Screen {
        background: $background;
    }
    
    .title {
        text-align: center;
        text-style: bold;
        color: $primary;
        margin: 1;
    }
    
    .panel-title {
        text-style: bold;
        color: $primary-lighten-2;
        margin-bottom: 1;
    }
    """
    
    BINDINGS = [
        Binding("ctrl+c", "quit", "çµ‚äº†", show=False),
    ]
    
    def __init__(self, config, args, tokenizer):
        super().__init__()
        self.config = config
        self.args = args
        self.tokenizer = tokenizer
        
        # é¸æŠã•ã‚ŒãŸå€¤ã‚’ä¿å­˜
        self.selected_category = None
        self.selected_prompt = None
        self.selected_preset_info = None
        self.selected_epochs = config.num_epochs
        self.selected_generation_method = "template"
        self.selected_generation_model = "llama3"
        self.selected_refinement_model = "llama3"
        
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ç”¨
        self.selected_model_path = None
        self.selected_model_info = None
        
    async def on_mount(self) -> None:
        """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®å‡¦ç†"""
        self.title = "Text2GPT1 Textual Edition"
        self.sub_title = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ GPTã‚’è‡ªå‹•ç”Ÿæˆ"
        
        if self.args.inference and self.args.model_path:
            # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•
            self.selected_model_path = self.args.model_path
            info_path = os.path.join(self.args.model_path, "generation_info.json")
            if os.path.exists(info_path):
                with open(info_path, 'r', encoding='utf-8') as f:
                    self.selected_model_info = json.load(f)
            await self.push_screen(InferenceScreen())
        elif self.args.prompt:
            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            self.selected_prompt = self.args.prompt
            
            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ç”Ÿæˆæ–¹æ³•ã¨ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š
            if not self.args.no_ollama:
                self.selected_generation_method = "ollama"
                self.selected_generation_model = self.args.ollama_gen_model
                self.selected_refinement_model = self.args.ollama_refine_model
            else:
                self.selected_generation_method = "template"
            
            await self.push_screen(MainProcessScreen())
        else:
            # å¯¾è©±å½¢å¼ã§é–‹å§‹ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰
            await self.push_screen(MainMenuScreen())
            
    def action_quit(self) -> None:
        """ã‚¢ãƒ—ãƒªã‚’çµ‚äº†"""
        self.exit()


# ===== 15. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ =====
def create_parser():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½œæˆ"""
    parser = argparse.ArgumentParser(
        description='Text2GPT1 Textualç‰ˆ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‰¹å®šã®æ€§è³ªã‚’æŒã¤GPT-1ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆ',
        epilog='''
ä½¿ç”¨ä¾‹:
  # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ï¼ˆå¯¾è©±å½¢å¼ with ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰
  %(prog)s
  
  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œï¼ˆéå¯¾è©±å½¢å¼ï¼‰
  %(prog)s --prompt "æ–™ç†ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹æ¥½ã—ã„GPT"
  
  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰
  %(prog)s --inference --model-path models/cooking_recipe_generator_20240123_143022
  
  # TUIã‚’ç„¡åŠ¹åŒ–
  %(prog)s --no-tui
  
  # Ollamaã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  %(prog)s --prompt "è©©çš„ãªæ–‡ç« ã‚’æ›¸ãGPT" --ollama-gen-model llama3 --ollama-refine-model mistral
  
  # Ollamaã‚’ä½¿ã‚ãšãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ
  %(prog)s --prompt "æŠ€è¡“æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹GPT" --no-ollama
  
  # ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®å®Ÿè¡Œ
  %(prog)s --prompt "ä¼šè©±ãŒå¾—æ„ãªGPT" --initial-tokens 3000 --final-tokens 1500 --epochs 50

å¿…è¦ãªæº–å‚™:
  1. PyTorchã¨Transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install torch transformers safetensors tqdm numpy requests
  2. Textualã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install textual
  3. (æ¨å¥¨) Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èµ·å‹•: ollama serve
  4. (æ¨å¥¨) Ollamaãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: ollama pull llama3
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '-p', '--prompt',
        type=str,
        help='ç”Ÿæˆã—ãŸã„GPTã®æ€§è³ªã‚’æŒ‡å®šã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ'
    )
    
    parser.add_argument(
        '--inference',
        action='store_true',
        help='æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•'
    )
    
    parser.add_argument(
        '--model-path',
        type=str,
        help='æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹'
    )
    
    parser.add_argument(
        '--no-tui',
        action='store_true',
        help='TUIï¼ˆText User Interfaceï¼‰ã‚’ç„¡åŠ¹åŒ–'
    )
    
    data_group = parser.add_argument_group('ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³')
    data_group.add_argument(
        '--initial-tokens',
        type=int,
        default=2000,
        help='åˆæœŸç”Ÿæˆã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2000ï¼‰'
    )
    data_group.add_argument(
        '--final-tokens',
        type=int,
        default=1000,
        help='ç²¾é¸å¾Œã®æœ€çµ‚ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰'
    )
    data_group.add_argument(
        '--no-ollama',
        action='store_true',
        help='Ollamaã‚’ä½¿ç”¨ã›ãšã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ'
    )
    data_group.add_argument(
        '--ollama-model',
        type=str,
        default='llama3',
        help='[éæ¨å¥¨] --ollama-gen-modelã¨--ollama-refine-modelã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„'
    )
    data_group.add_argument(
        '--ollama-gen-model',
        type=str,
        default='llama3',
        help='ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: llama3ï¼‰'
    )
    data_group.add_argument(
        '--ollama-refine-model',
        type=str,
        default=None,
        help='ãƒ‡ãƒ¼ã‚¿ç²¾é¸ã«ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç”Ÿæˆã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ï¼‰'
    )
    data_group.add_argument(
        '--ollama-host',
        type=str,
        default='http://localhost:11434',
        help='Ollamaã‚µãƒ¼ãƒãƒ¼ã®URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: http://localhost:11434ï¼‰'
    )
    
    train_group = parser.add_argument_group('å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³')
    train_group.add_argument(
        '--epochs',
        type=int,
        default=20,
        help='å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰'
    )
    train_group.add_argument(
        '--batch-size',
        type=int,
        default=2,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰'
    )
    train_group.add_argument(
        '--learning-rate',
        type=float,
        default=3e-4,
        help='å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3e-4ï¼‰'
    )
    
    model_group = parser.add_argument_group('ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³')
    model_group.add_argument(
        '--model-size',
        type=str,
        choices=['12M', '33M', '117M'],
        default='12M',
        help='ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 12Mï¼‰'
    )
    
    output_group = parser.add_argument_group('å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³')
    output_group.add_argument(
        '--output-dir',
        type=str,
        default='models',
        help='ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: modelsï¼‰'
    )
    
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help='è©³ç´°ãªãƒ­ã‚°ã‚’è¡¨ç¤º'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 42ï¼‰'
    )
    parser.add_argument(
        '--version',
        action='version',
        version='%(prog)s 6.2.0 (Textual Edition with Inference)'
    )
    
    return parser


# ===== 16. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° =====
def main():
    """Text2GPT1ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = create_parser()
    args = parser.parse_args()
    
    # å¤ã„--ollama-modelãŒæŒ‡å®šã•ã‚Œã¦ã„ã¦æ–°ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã®äº’æ›æ€§å‡¦ç†
    if args.ollama_model != 'llama3' and args.ollama_gen_model == 'llama3':
        args.ollama_gen_model = args.ollama_model
    if args.ollama_refine_model is None:
        args.ollama_refine_model = args.ollama_gen_model
    
    print("="*60)
    print("Text2GPT1 - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ GPTã‚’è‡ªå‹•ç”Ÿæˆ")
    print("Version 6.2.0 - Textual Edition with Inference")
    print("="*60)
    
    config = Config()
    config.seed = args.seed
    config.num_epochs = args.epochs
    config.batch_size = args.batch_size
    config.learning_rate = args.learning_rate
    config.initial_tokens = args.initial_tokens
    config.final_tokens = args.final_tokens
    
    if args.model_size == '33M':
        config.n_embd = 512
        config.n_layer = 8
        config.n_head = 8
    elif args.model_size == '117M':
        config.n_embd = 768
        config.n_layer = 12
        config.n_head = 12
    
    torch.manual_seed(config.seed)
    
    print("\nãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æº–å‚™ä¸­...")
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    use_textual_tui = not args.prompt and not args.no_tui and not (args.inference and args.model_path)
    
    if use_textual_tui:
        # Textual TUIãƒ¢ãƒ¼ãƒ‰
        try:
            app = Text2GPT1App(config, args, tokenizer)
            app.run()
        except KeyboardInterrupt:
            print("\n\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
    else:
        # éTUIãƒ¢ãƒ¼ãƒ‰
        if args.inference and args.model_path:
            # æ¨è«–ãƒ¢ãƒ¼ãƒ‰
            print(f"\næ¨è«–ãƒ¢ãƒ¼ãƒ‰: {args.model_path}")
            
            try:
                model, tokenizer, config = load_model_safetensors(args.model_path)
                print("ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
                
                # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ¨è«–ãƒ«ãƒ¼ãƒ—
                print("\nãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                while True:
                    prompt = input("\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ> ").strip()
                    if prompt.lower() == 'quit':
                        break
                    
                    if not prompt:
                        continue
                    
                    generated = generate_sample(
                        model, tokenizer, config,
                        prompt=prompt,
                        max_length=100,
                        temperature=0.8,
                        top_k=40
                    )
                    
                    print(f"\nç”Ÿæˆçµæœ:\n{generated}")
                    
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {e}")
                
        elif args.prompt:
            # éå¯¾è©±çš„ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            user_prompt = args.prompt
            print(f"\nç›®æ¨™: ã€Œ{user_prompt}ã€GPTã‚’ä½œæˆ")
            
            # ç”Ÿæˆæ–¹æ³•ã®è¨­å®š
            generation_method = "template" if args.no_ollama else "ollama"
            
            folder_path, domain = run_main_process(
                config, args, user_prompt, tokenizer,
                generation_method=generation_method,
                generation_model=args.ollama_gen_model,
                refinement_model=args.ollama_refine_model
            )
            
            print("\n"+"="*60)
            print("Text2GPT1 å®Œäº†ï¼")
            print(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€: {folder_path}")
            print(f"æ€§è³ª: {user_prompt}")
            print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
            print(f"ã‚µã‚¤ã‚º: {args.model_size}")
            print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {folder_path}/dataset.json")
            print(f"ç”Ÿæˆæ–¹æ³•: {generation_method}")
            if generation_method == "ollama":
                print(f"ç”Ÿæˆãƒ¢ãƒ‡ãƒ«: {args.ollama_gen_model}")
                print(f"ç²¾é¸ãƒ¢ãƒ‡ãƒ«: {args.ollama_refine_model}")
            print("="*60)
            
        else:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—ã®éTUIãƒ¢ãƒ¼ãƒ‰
            print("\nã©ã®ã‚ˆã†ãªæ€§è³ªã®GPT-1ã‚’ç”Ÿæˆã—ã¾ã™ã‹ï¼Ÿ")
            print("\nã‚«ãƒ†ã‚´ãƒª:")
            for key, cat in CATEGORIES.items():
                print(f"  - {cat['name']} ({cat['name_ja']}): {cat['description']}")
            print("\nä¾‹: æ–™ç†ãƒ¬ã‚·ãƒ”ã‚’ç”Ÿæˆã™ã‚‹ã€è©©çš„ãªæ–‡ç« ã‚’æ›¸ãã€æŠ€è¡“æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹")
            user_prompt = input("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ").strip()
            
            if not user_prompt:
                user_prompt = "ä¸€èˆ¬çš„ãªæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹"
                print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨: {user_prompt}")
            
            print(f"\nç›®æ¨™: ã€Œ{user_prompt}ã€GPTã‚’ä½œæˆ")
            
            # ç”Ÿæˆæ–¹æ³•ã®è¨­å®š
            generation_method = "template" if args.no_ollama else "ollama"
            
            folder_path, domain = run_main_process(
                config, args, user_prompt, tokenizer,
                generation_method=generation_method,
                generation_model=args.ollama_gen_model,
                refinement_model=args.ollama_refine_model
            )
            
            print("\n"+"="*60)
            print("Text2GPT1 å®Œäº†ï¼")
            print(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€: {folder_path}")
            print(f"æ€§è³ª: {user_prompt}")
            print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
            print(f"ã‚µã‚¤ã‚º: {args.model_size}")
            print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {folder_path}/dataset.json")
            print(f"ç”Ÿæˆæ–¹æ³•: {generation_method}")
            if generation_method == "ollama":
                print(f"ç”Ÿæˆãƒ¢ãƒ‡ãƒ«: {args.ollama_gen_model}")
                print(f"ç²¾é¸ãƒ¢ãƒ‡ãƒ«: {args.ollama_refine_model}")
            print("="*60)


if __name__ == "__main__":
    main()